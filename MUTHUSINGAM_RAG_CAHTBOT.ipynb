{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MAIN CODE\n"
      ],
      "metadata": {
        "id": "qxfseyqz5y3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG9z5QVWxJzZ",
        "outputId": "f3a51283-17e0-42f6-b34f-a526c0b3b618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.4)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Requirement already satisfied: faster-whisper in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.29.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.21.1)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (1.21.0)\n",
            "Requirement already satisfied: av>=11 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (14.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: youtube_transcript_api in /usr/local/lib/python3.11/dist-packages (1.0.2)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.11/dist-packages (15.0.0)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube_transcript_api) (0.7.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Loading Summarization Model...\n",
            "üîπ Loading Embedding Model...\n",
            "üîπ Loading Whisper Model...\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled4.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1AT_rbhDvfCM7lYmCAbLiMYJH6FL1qPrp\n",
        "\"\"\"\n",
        "##\n",
        "!pip install torch transformers faiss-cpu sentence-transformers pdfplumber\n",
        "\n",
        "!pip install pymupdf\n",
        "\n",
        "!pip install python-docx\n",
        "\n",
        "!pip install pytesseract\n",
        "\n",
        "!pip install openai-whisper\n",
        "\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install faster-whisper\n",
        "!pip install torch\n",
        "\n",
        "!pip install beautifulsoup4 requests youtube_transcript_api pytube openai-whisper moviepy\n",
        "\n",
        "import os\n",
        "import re\n",
        "import faiss\n",
        "import requests\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytesseract\n",
        "import faster_whisper\n",
        "from bs4 import BeautifulSoup\n",
        "from pytube import YouTube\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from PIL import Image\n",
        "from docx import Document\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import mimetypes\n",
        "import nltk\n",
        "import torch\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Load Summarization Model (Mistral-7B-Instruct via Hugging Face)\n",
        "print(\"üîπ Loading Summarization Model...\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n",
        "# Load Sentence Embedding Model (Efficient Retrieval)\n",
        "print(\"üîπ Loading Embedding Model...\")\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Load Faster Whisper for Video Transcription\n",
        "print(\"üîπ Loading Whisper Model...\")\n",
        "whisper_model = faster_whisper.WhisperModel(\"base\")\n",
        "\n",
        "# FAISS Index Initialization\n",
        "dimension = 384  # MiniLM embedding size\n",
        "faiss_index = faiss.IndexFlatL2(dimension)\n",
        "documents = []  # Stores extracted sentences\n",
        "\n",
        "# üü¢ Function to Extract Text from Web Pages\n",
        "def extract_text_from_web(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract text from paragraphs\n",
        "        paragraphs = soup.find_all('p')\n",
        "        text = \"\\n\".join([para.get_text() for para in paragraphs])\n",
        "\n",
        "        return text.strip() if text.strip() else \"No meaningful text found on the webpage.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting text from {url}: {str(e)}\"\n",
        "\n",
        "# üü¢ Function to Extract Subtitles from YouTube Videos\n",
        "def extract_text_from_youtube(url):\n",
        "    try:\n",
        "        video_id = url.split(\"v=\")[-1]\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        text = \" \".join([t['text'] for t in transcript])\n",
        "\n",
        "        return text.strip() if text.strip() else \"No subtitles found for this YouTube video.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error extracting subtitles from YouTube: {str(e)}\"\n",
        "\n",
        "# üü¢ Function to Process File or URL\n",
        "def process_input(source):\n",
        "    if source.startswith(\"http\"):\n",
        "        if \"youtube.com\" in source or \"youtu.be\" in source:\n",
        "            extracted_text = extract_text_from_youtube(source)\n",
        "        else:\n",
        "            extracted_text = extract_text_from_web(source)\n",
        "    else:\n",
        "        extracted_text = process_file(source)\n",
        "\n",
        "    if isinstance(extracted_text, str) and extracted_text.strip():\n",
        "        return extracted_text\n",
        "    return None\n",
        "\n",
        "# üü¢ Function to Extract and Clean Text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text + \"\\n\"\n",
        "\n",
        "    # Clean extracted text\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    text = re.sub(r\"[0-9]+\\.\\d+|\\d+\\.\\s?\", \"\", text)  # Remove section numbers\n",
        "    return text.strip()\n",
        "\n",
        "# üü¢ Function to Extract Text from DOCX\n",
        "def extract_text_from_docx(docx_path):\n",
        "    doc = Document(docx_path)\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return text.strip()\n",
        "\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "# Load BLIP model and processor globally (to avoid reloading for every image)\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "def extract_text_from_image(file_path):\n",
        "    text = pytesseract.image_to_string(Image.open(file_path)).strip()\n",
        "\n",
        "    if text:\n",
        "        return text  # ‚úÖ Return a string instead of a dictionary\n",
        "\n",
        "    print(f\"‚ö†Ô∏è No readable text found in {file_path}. Using vision model (BLIP).\")\n",
        "\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    image = Image.open(file_path).convert(\"RGB\")\n",
        "    inputs = processor(image, return_tensors=\"pt\")\n",
        "    output = model.generate(**inputs)\n",
        "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return f\"[Vision Model]: {caption}\"  # ‚úÖ Always return a string\n",
        "\n",
        "import os\n",
        "import moviepy.editor as mp\n",
        "import whisper\n",
        "def extract_text_from_video(file_path):\n",
        "    try:\n",
        "        audio_path = \"temp_audio.wav\"\n",
        "        video = mp.VideoFileClip(file_path)\n",
        "        video.audio.write_audiofile(audio_path)\n",
        "\n",
        "        model = whisper.load_model(\"base\")\n",
        "        result = model.transcribe(audio_path)\n",
        "\n",
        "        os.remove(audio_path)\n",
        "        return result['text'] if result['text'].strip() else \"No meaningful audio detected.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing video: {str(e)}\"\n",
        "\n",
        "# üü¢ Function to Add Extracted Text to FAISS Index\n",
        "def add_document(text):\n",
        "    global faiss_index, documents\n",
        "    sentences = sent_tokenize(text)  # Better sentence splitting\n",
        "    embeddings = embedding_model.encode(sentences, convert_to_numpy=True)\n",
        "    faiss_index.add(embeddings)\n",
        "    documents.extend(sentences)\n",
        "\n",
        "def process_file(file_path):\n",
        "    mime_type, _ = mimetypes.guess_type(file_path)\n",
        "\n",
        "    if not mime_type:\n",
        "        print(f\"‚ùå Could not determine file type: {file_path}\")\n",
        "        return None\n",
        "\n",
        "    if \"pdf\" in mime_type:\n",
        "        extracted_text = extract_text_from_pdf(file_path)\n",
        "    elif \"word\" in mime_type or file_path.lower().endswith(\".docx\"):\n",
        "        extracted_text = extract_text_from_docx(file_path)\n",
        "    elif \"csv\" in mime_type:\n",
        "        extracted_text = process_csv(file_path)\n",
        "    elif mime_type.startswith(\"image\"):\n",
        "        extracted_text = extract_text_from_image(file_path)\n",
        "    elif mime_type.startswith(\"video\") or mime_type.startswith(\"audio\"):\n",
        "        extracted_text = extract_text_from_video(file_path)\n",
        "    else:\n",
        "        print(f\"‚ùå Unsupported file type: {mime_type}\")\n",
        "        return None\n",
        "\n",
        "    # ‚úÖ Fix: Check if extracted_text is a dictionary before calling strip()\n",
        "    if isinstance(extracted_text, dict):\n",
        "        if \"Extracted Text\" in extracted_text and extracted_text[\"Extracted Text\"].strip():\n",
        "            return extracted_text\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è No meaningful text extracted from {file_path}. Returning description.\")\n",
        "            return extracted_text  # Return dictionary response as is\n",
        "\n",
        "    elif isinstance(extracted_text, str):\n",
        "        return extracted_text.strip() if extracted_text.strip() else None\n",
        "\n",
        "    return None  # Handle unexpected cases\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def process_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    summary = f\"üîπ CSV File Summary:\\n\"\n",
        "    summary += f\"- Total Rows: {len(df)}\\n\"\n",
        "    summary += f\"- Total Columns: {len(df.columns)}\\n\"\n",
        "    summary += f\"- Column Names: {', '.join(df.columns)}\\n\"\n",
        "\n",
        "    # Column Data Types\n",
        "    col_types = df.dtypes.astype(str).to_dict()\n",
        "    summary += f\"- Column Types:\\n\" + \"\\n\".join([f\"  ‚Ä¢ {col}: {dtype}\" for col, dtype in col_types.items()]) + \"\\n\"\n",
        "\n",
        "    # Numeric Summary\n",
        "    num_cols = df.select_dtypes(include=['number']).columns\n",
        "    if len(num_cols) > 0:\n",
        "        num_summary = df[num_cols].describe().to_dict()\n",
        "        summary += \"\\nüîπ Numerical Summary:\\n\"\n",
        "        for col, stats in num_summary.items():\n",
        "            summary += f\"  ‚Ä¢ {col}: {stats}\\n\"\n",
        "\n",
        "    # Categorical Summary\n",
        "    cat_cols = df.select_dtypes(include=['object']).columns\n",
        "    if len(cat_cols) > 0:\n",
        "        summary += \"\\nüîπ Categorical Summary:\\n\"\n",
        "        for col in cat_cols:\n",
        "            top_values = df[col].value_counts().head(3).to_dict()  # Show top 3 frequent values\n",
        "            summary += f\"  ‚Ä¢ {col}: {top_values}\\n\"\n",
        "\n",
        "    return summary  # ‚úÖ Always return a string instead of a dictionary\n",
        "\n",
        "def retrieve_top_k(query, k=3):\n",
        "    if not documents:\n",
        "        return \"‚ùå No documents available for retrieval.\"\n",
        "\n",
        "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
        "    _, indices = faiss_index.search(query_embedding, k)\n",
        "\n",
        "    retrieved_texts = [documents[idx] for idx in indices[0] if idx < len(documents)]\n",
        "\n",
        "    if not retrieved_texts:\n",
        "        return \"‚ùå No relevant context found.\"\n",
        "\n",
        "    return \"\\n\".join(retrieved_texts)\n",
        "\n",
        "def generate_summary(text):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradients for faster inference\n",
        "        summary_ids = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=150,\n",
        "            min_length=50,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# üîπ RAG Chatbot with File & Web Scraping Support\n",
        "def chatbot():\n",
        "    global faiss_index, documents\n",
        "\n",
        "    faiss_index = faiss.IndexFlatL2(dimension)\n",
        "    documents = []\n",
        "\n",
        "    print(\"\\nüîπ RAG Chatbot - Upload Files or URLs\")\n",
        "\n",
        "    while True:\n",
        "        source = input(\"Enter file path or URL (or 'done' to finish): \").strip()\n",
        "        if source.lower() == \"done\":\n",
        "            break\n",
        "\n",
        "        print(f\"‚úÖ Processing {source} ...\")\n",
        "        extracted_text = process_input(source)\n",
        "\n",
        "        if extracted_text:\n",
        "            add_document(extracted_text)\n",
        "            print(f\"‚úÖ Successfully processed {source}.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Failed to process {source}.\")\n",
        "\n",
        "    print(\"\\n‚úÖ Index Built Successfully!\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nüîç Enter your query (or 'exit' to quit): \").strip()\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "\n",
        "        retrieved_text = retrieve_top_k(query)\n",
        "        print(\"\\nüìå **Top Retrieved Contexts:**\")\n",
        "        print(retrieved_text)\n",
        "\n",
        "        print(\"\\nüìå **Abstractive Summary:**\")\n",
        "        summary = generate_summary(retrieved_text)\n",
        "        print(summary)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV FILE INPUT\n"
      ],
      "metadata": {
        "id": "LP9Uxjh455qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMZ6IWD61OCm",
        "outputId": "59f5dd56-4542-4dc0-f814-bf92980a21b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): /content/Students_Grading_Dataset.csv\n",
            "‚úÖ Processing /content/Students_Grading_Dataset.csv ...\n",
            "‚úÖ Successfully processed /content/Students_Grading_Dataset.csv.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): describe oit\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "üîπ CSV File Summary:\n",
            "- Total Rows: 5000\n",
            "- Total Columns: 23\n",
            "- Column Names: Student_ID, First_Name, Last_Name, Email, Gender, Age, Department, Attendance (%), Midterm_Score, Final_Score, Assignments_Avg, Quizzes_Avg, Participation_Score, Projects_Score, Total_Score, Grade, Study_Hours_per_Week, Extracurricular_Activities, Internet_Access_at_Home, Parent_Education_Level, Family_Income_Level, Stress_Level (1-10), Sleep_Hours_per_Night\n",
            "- Column Types:\n",
            "  ‚Ä¢ Student_ID: object\n",
            "  ‚Ä¢ First_Name: object\n",
            "  ‚Ä¢ Last_Name: object\n",
            "  ‚Ä¢ Email: object\n",
            "  ‚Ä¢ Gender: object\n",
            "  ‚Ä¢ Age: int64\n",
            "  ‚Ä¢ Department: object\n",
            "  ‚Ä¢ Attendance (%): float64\n",
            "  ‚Ä¢ Midterm_Score: float64\n",
            "  ‚Ä¢ Final_Score: float64\n",
            "  ‚Ä¢ Assignments_Avg: float64\n",
            "  ‚Ä¢ Quizzes_Avg: float64\n",
            "  ‚Ä¢ Participation_Score: float64\n",
            "  ‚Ä¢ Projects_Score: float64\n",
            "  ‚Ä¢ Total_Score: float64\n",
            "  ‚Ä¢ Grade: object\n",
            "  ‚Ä¢ Study_Hours_per_Week: float64\n",
            "  ‚Ä¢ Extracurricular_Activities: object\n",
            "  ‚Ä¢ Internet_Access_at_Home: object\n",
            "  ‚Ä¢ Parent_Education_Level: object\n",
            "  ‚Ä¢ Family_Income_Level: object\n",
            "  ‚Ä¢ Stress_Level (1-10): int64\n",
            "  ‚Ä¢ Sleep_Hours_per_Night: float64\n",
            "\n",
            "üîπ Numerical Summary:\n",
            "  ‚Ä¢ Age: {'count': 5000.0, 'mean': 21.0484, 'std': 1.9897862422526467, 'min': 18.0, '25%': 19.0, '50%': 21.0, '75%': 23.0, 'max': 24.0}\n",
            "  ‚Ä¢ Attendance (%): {'count': 4484.0, 'mean': 75.431409455843, 'std': 14.372445606230198, 'min': 50.01, '25%': 63.265, '50%': 75.725, '75%': 87.4725, 'max': 100.0}\n",
            "  ‚Ä¢ Midterm_Score: {'count': 5000.0, 'mean': 70.326844, 'std': 17.213208523375233, 'min': 40.0, '25%': 55.4575, '50%': 70.50999999999999, '75%': 84.97, 'max': 99.98}\n",
            "  ‚Ä¢ Final_Score: {'count': 5000.0, 'mean': 69.64078799999999, 'std': 17.238744216429662, 'min': 40.0, '25%': 54.667500000000004, '50%': 69.735, '75%': 84.5, 'max': 99.98}\n",
            "  ‚Ä¢ Assignments_Avg: {'count': 4483.0, 'mean': 74.79867276377426, 'std': 14.411799371836947, 'min': 50.0, '25%': 62.09, '50%': 74.81, '75%': 86.97, 'max': 99.98}\n",
            "  ‚Ä¢ Quizzes_Avg: {'count': 5000.0, 'mean': 74.910728, 'std': 14.504280651637298, 'min': 50.03, '25%': 62.49, '50%': 74.695, '75%': 87.63, 'max': 99.96}\n",
            "  ‚Ä¢ Participation_Score: {'count': 5000.0, 'mean': 4.980024, 'std': 2.8901360204439057, 'min': 0.0, '25%': 2.44, '50%': 4.955, '75%': 7.5, 'max': 10.0}\n",
            "  ‚Ä¢ Projects_Score: {'count': 5000.0, 'mean': 74.92485999999998, 'std': 14.423414658064832, 'min': 50.01, '25%': 62.32, '50%': 74.98, '75%': 87.3675, 'max': 100.0}\n",
            "  ‚Ä¢ Total_Score: {'count': 5000.0, 'mean': 75.121804, 'std': 14.399941391825209, 'min': 50.02, '25%': 62.835, '50%': 75.39500000000001, '75%': 87.6525, 'max': 99.99}\n",
            "  ‚Ä¢ Study_Hours_per_Week: {'count': 5000.0, 'mean': 17.658859999999997, 'std': 7.275864288222276, 'min': 5.0, '25%': 11.4, '50%': 17.5, '75%': 24.1, 'max': 30.0}\n",
            "  ‚Ä¢ Stress_Level (1-10): {'count': 5000.0, 'mean': 5.4808, 'std': 2.8615501138038337, 'min': 1.0, '25%': 3.0, '50%': 5.0, '75%': 8.0, 'max': 10.0}\n",
            "  ‚Ä¢ Sleep_Hours_per_Night: {'count': 5000.0, 'mean': 6.4881400000000005, 'std': 1.4522834316458921, 'min': 4.0, '25%': 5.2, '50%': 6.5, '75%': 7.7, 'max': 9.0}\n",
            "\n",
            "üîπ Categorical Summary:\n",
            "  ‚Ä¢ Student_ID: {'S5999': 1, 'S1000': 1, 'S1001': 1}\n",
            "  ‚Ä¢ First_Name: {'Maria': 657, 'Ahmed': 651, 'Ali': 644}\n",
            "  ‚Ä¢ Last_Name: {'Johnson': 868, 'Jones': 850, 'Davis': 829}\n",
            "  ‚Ä¢ Email: {'student4999@university.com': 1, 'student0@university.com': 1, 'student1@university.com': 1}\n",
            "  ‚Ä¢ Gender: {'Male': 2551, 'Female': 2449}\n",
            "  ‚Ä¢ Department: {'CS': 2022, 'Engineering': 1469, 'Business': 1006}\n",
            "  ‚Ä¢ Grade: {'A': 1495, 'B': 978, 'D': 889}\n",
            "  ‚Ä¢ Extracurricular_Activities: {'No': 3493, 'Yes': 1507}\n",
            "  ‚Ä¢ Internet_Access_at_Home: {'Yes': 4485, 'No': 515}\n",
            "  ‚Ä¢ Parent_Education_Level: {'PhD': 820, \"Bachelor's\": 810, 'High School': 796}\n",
            "  ‚Ä¢ Family_Income_Level: {'Low': 1983, 'Medium': 1973, 'High': 1044}\n",
            "üîπ CSV File Summary:\n",
            "- Total Rows: 5000\n",
            "- Total Columns: 23\n",
            "- Column Names: Student_ID, First_Name, Last_Name, Email, Gender, Age, Department, Attendance (%), Midterm_Score, Final_Score, Assignments_Avg, Quizzes_Avg, Participation_Score, Projects_Score, Total_Score, Grade, Study_Hours_per_Week, Extracurricular_Activities, Internet_Access_at_Home, Parent_Education_Level, Family_Income_Level, Stress_Level (1-10), Sleep_Hours_per_Night\n",
            "- Column Types:\n",
            "  ‚Ä¢ Student_ID: object\n",
            "  ‚Ä¢ First_Name: object\n",
            "  ‚Ä¢ Last_Name: object\n",
            "  ‚Ä¢ Email: object\n",
            "  ‚Ä¢ Gender: object\n",
            "  ‚Ä¢ Age: int64\n",
            "  ‚Ä¢ Department: object\n",
            "  ‚Ä¢ Attendance (%): float64\n",
            "  ‚Ä¢ Midterm_Score: float64\n",
            "  ‚Ä¢ Final_Score: float64\n",
            "  ‚Ä¢ Assignments_Avg: float64\n",
            "  ‚Ä¢ Quizzes_Avg: float64\n",
            "  ‚Ä¢ Participation_Score: float64\n",
            "  ‚Ä¢ Projects_Score: float64\n",
            "  ‚Ä¢ Total_Score: float64\n",
            "  ‚Ä¢ Grade: object\n",
            "  ‚Ä¢ Study_Hours_per_Week: float64\n",
            "  ‚Ä¢ Extracurricular_Activities: object\n",
            "  ‚Ä¢ Internet_Access_at_Home: object\n",
            "  ‚Ä¢ Parent_Education_Level: object\n",
            "  ‚Ä¢ Family_Income_Level: object\n",
            "  ‚Ä¢ Stress_Level (1-10): int64\n",
            "  ‚Ä¢ Sleep_Hours_per_Night: float64\n",
            "\n",
            "üîπ Numerical Summary:\n",
            "  ‚Ä¢ Age: {'count': 5000.0, 'mean': 21.0484, 'std': 1.9897862422526467, 'min': 18.0, '25%': 19.0, '50%': 21.0, '75%': 23.0, 'max': 24.0}\n",
            "  ‚Ä¢ Attendance (%): {'count': 4484.0, 'mean': 75.431409455843, 'std': 14.372445606230198, 'min': 50.01, '25%': 63.265, '50%': 75.725, '75%': 87.4725, 'max': 100.0}\n",
            "  ‚Ä¢ Midterm_Score: {'count': 5000.0, 'mean': 70.326844, 'std': 17.213208523375233, 'min': 40.0, '25%': 55.4575, '50%': 70.50999999999999, '75%': 84.97, 'max': 99.98}\n",
            "  ‚Ä¢ Final_Score: {'count': 5000.0, 'mean': 69.64078799999999, 'std': 17.238744216429662, 'min': 40.0, '25%': 54.667500000000004, '50%': 69.735, '75%': 84.5, 'max': 99.98}\n",
            "  ‚Ä¢ Assignments_Avg: {'count': 4483.0, 'mean': 74.79867276377426, 'std': 14.411799371836947, 'min': 50.0, '25%': 62.09, '50%': 74.81, '75%': 86.97, 'max': 99.98}\n",
            "  ‚Ä¢ Quizzes_Avg: {'count': 5000.0, 'mean': 74.910728, 'std': 14.504280651637298, 'min': 50.03, '25%': 62.49, '50%': 74.695, '75%': 87.63, 'max': 99.96}\n",
            "  ‚Ä¢ Participation_Score: {'count': 5000.0, 'mean': 4.980024, 'std': 2.8901360204439057, 'min': 0.0, '25%': 2.44, '50%': 4.955, '75%': 7.5, 'max': 10.0}\n",
            "  ‚Ä¢ Projects_Score: {'count': 5000.0, 'mean': 74.92485999999998, 'std': 14.423414658064832, 'min': 50.01, '25%': 62.32, '50%': 74.98, '75%': 87.3675, 'max': 100.0}\n",
            "  ‚Ä¢ Total_Score: {'count': 5000.0, 'mean': 75.121804, 'std': 14.399941391825209, 'min': 50.02, '25%': 62.835, '50%': 75.39500000000001, '75%': 87.6525, 'max': 99.99}\n",
            "  ‚Ä¢ Study_Hours_per_Week: {'count': 5000.0, 'mean': 17.658859999999997, 'std': 7.275864288222276, 'min': 5.0, '25%': 11.4, '50%': 17.5, '75%': 24.1, 'max': 30.0}\n",
            "  ‚Ä¢ Stress_Level (1-10): {'count': 5000.0, 'mean': 5.4808, 'std': 2.8615501138038337, 'min': 1.0, '25%': 3.0, '50%': 5.0, '75%': 8.0, 'max': 10.0}\n",
            "  ‚Ä¢ Sleep_Hours_per_Night: {'count': 5000.0, 'mean': 6.4881400000000005, 'std': 1.4522834316458921, 'min': 4.0, '25%': 5.2, '50%': 6.5, '75%': 7.7, 'max': 9.0}\n",
            "\n",
            "üîπ Categorical Summary:\n",
            "  ‚Ä¢ Student_ID: {'S5999': 1, 'S1000': 1, 'S1001': 1}\n",
            "  ‚Ä¢ First_Name: {'Maria': 657, 'Ahmed': 651, 'Ali': 644}\n",
            "  ‚Ä¢ Last_Name: {'Johnson': 868, 'Jones': 850, 'Davis': 829}\n",
            "  ‚Ä¢ Email: {'student4999@university.com': 1, 'student0@university.com': 1, 'student1@university.com': 1}\n",
            "  ‚Ä¢ Gender: {'Male': 2551, 'Female': 2449}\n",
            "  ‚Ä¢ Department: {'CS': 2022, 'Engineering': 1469, 'Business': 1006}\n",
            "  ‚Ä¢ Grade: {'A': 1495, 'B': 978, 'D': 889}\n",
            "  ‚Ä¢ Extracurricular_Activities: {'No': 3493, 'Yes': 1507}\n",
            "  ‚Ä¢ Internet_Access_at_Home: {'Yes': 4485, 'No': 515}\n",
            "  ‚Ä¢ Parent_Education_Level: {'PhD': 820, \"Bachelor's\": 810, 'High School': 796}\n",
            "  ‚Ä¢ Family_Income_Level: {'Low': 1983, 'Medium': 1973, 'High': 1044}\n",
            "üîπ CSV File Summary:\n",
            "- Total Rows: 5000\n",
            "- Total Columns: 23\n",
            "- Column Names: Student_ID, First_Name, Last_Name, Email, Gender, Age, Department, Attendance (%), Midterm_Score, Final_Score, Assignments_Avg, Quizzes_Avg, Participation_Score, Projects_Score, Total_Score, Grade, Study_Hours_per_Week, Extracurricular_Activities, Internet_Access_at_Home, Parent_Education_Level, Family_Income_Level, Stress_Level (1-10), Sleep_Hours_per_Night\n",
            "- Column Types:\n",
            "  ‚Ä¢ Student_ID: object\n",
            "  ‚Ä¢ First_Name: object\n",
            "  ‚Ä¢ Last_Name: object\n",
            "  ‚Ä¢ Email: object\n",
            "  ‚Ä¢ Gender: object\n",
            "  ‚Ä¢ Age: int64\n",
            "  ‚Ä¢ Department: object\n",
            "  ‚Ä¢ Attendance (%): float64\n",
            "  ‚Ä¢ Midterm_Score: float64\n",
            "  ‚Ä¢ Final_Score: float64\n",
            "  ‚Ä¢ Assignments_Avg: float64\n",
            "  ‚Ä¢ Quizzes_Avg: float64\n",
            "  ‚Ä¢ Participation_Score: float64\n",
            "  ‚Ä¢ Projects_Score: float64\n",
            "  ‚Ä¢ Total_Score: float64\n",
            "  ‚Ä¢ Grade: object\n",
            "  ‚Ä¢ Study_Hours_per_Week: float64\n",
            "  ‚Ä¢ Extracurricular_Activities: object\n",
            "  ‚Ä¢ Internet_Access_at_Home: object\n",
            "  ‚Ä¢ Parent_Education_Level: object\n",
            "  ‚Ä¢ Family_Income_Level: object\n",
            "  ‚Ä¢ Stress_Level (1-10): int64\n",
            "  ‚Ä¢ Sleep_Hours_per_Night: float64\n",
            "\n",
            "üîπ Numerical Summary:\n",
            "  ‚Ä¢ Age: {'count': 5000.0, 'mean': 21.0484, 'std': 1.9897862422526467, 'min': 18.0, '25%': 19.0, '50%': 21.0, '75%': 23.0, 'max': 24.0}\n",
            "  ‚Ä¢ Attendance (%): {'count': 4484.0, 'mean': 75.431409455843, 'std': 14.372445606230198, 'min': 50.01, '25%': 63.265, '50%': 75.725, '75%': 87.4725, 'max': 100.0}\n",
            "  ‚Ä¢ Midterm_Score: {'count': 5000.0, 'mean': 70.326844, 'std': 17.213208523375233, 'min': 40.0, '25%': 55.4575, '50%': 70.50999999999999, '75%': 84.97, 'max': 99.98}\n",
            "  ‚Ä¢ Final_Score: {'count': 5000.0, 'mean': 69.64078799999999, 'std': 17.238744216429662, 'min': 40.0, '25%': 54.667500000000004, '50%': 69.735, '75%': 84.5, 'max': 99.98}\n",
            "  ‚Ä¢ Assignments_Avg: {'count': 4483.0, 'mean': 74.79867276377426, 'std': 14.411799371836947, 'min': 50.0, '25%': 62.09, '50%': 74.81, '75%': 86.97, 'max': 99.98}\n",
            "  ‚Ä¢ Quizzes_Avg: {'count': 5000.0, 'mean': 74.910728, 'std': 14.504280651637298, 'min': 50.03, '25%': 62.49, '50%': 74.695, '75%': 87.63, 'max': 99.96}\n",
            "  ‚Ä¢ Participation_Score: {'count': 5000.0, 'mean': 4.980024, 'std': 2.8901360204439057, 'min': 0.0, '25%': 2.44, '50%': 4.955, '75%': 7.5, 'max': 10.0}\n",
            "  ‚Ä¢ Projects_Score: {'count': 5000.0, 'mean': 74.92485999999998, 'std': 14.423414658064832, 'min': 50.01, '25%': 62.32, '50%': 74.98, '75%': 87.3675, 'max': 100.0}\n",
            "  ‚Ä¢ Total_Score: {'count': 5000.0, 'mean': 75.121804, 'std': 14.399941391825209, 'min': 50.02, '25%': 62.835, '50%': 75.39500000000001, '75%': 87.6525, 'max': 99.99}\n",
            "  ‚Ä¢ Study_Hours_per_Week: {'count': 5000.0, 'mean': 17.658859999999997, 'std': 7.275864288222276, 'min': 5.0, '25%': 11.4, '50%': 17.5, '75%': 24.1, 'max': 30.0}\n",
            "  ‚Ä¢ Stress_Level (1-10): {'count': 5000.0, 'mean': 5.4808, 'std': 2.8615501138038337, 'min': 1.0, '25%': 3.0, '50%': 5.0, '75%': 8.0, 'max': 10.0}\n",
            "  ‚Ä¢ Sleep_Hours_per_Night: {'count': 5000.0, 'mean': 6.4881400000000005, 'std': 1.4522834316458921, 'min': 4.0, '25%': 5.2, '50%': 6.5, '75%': 7.7, 'max': 9.0}\n",
            "\n",
            "üîπ Categorical Summary:\n",
            "  ‚Ä¢ Student_ID: {'S5999': 1, 'S1000': 1, 'S1001': 1}\n",
            "  ‚Ä¢ First_Name: {'Maria': 657, 'Ahmed': 651, 'Ali': 644}\n",
            "  ‚Ä¢ Last_Name: {'Johnson': 868, 'Jones': 850, 'Davis': 829}\n",
            "  ‚Ä¢ Email: {'student4999@university.com': 1, 'student0@university.com': 1, 'student1@university.com': 1}\n",
            "  ‚Ä¢ Gender: {'Male': 2551, 'Female': 2449}\n",
            "  ‚Ä¢ Department: {'CS': 2022, 'Engineering': 1469, 'Business': 1006}\n",
            "  ‚Ä¢ Grade: {'A': 1495, 'B': 978, 'D': 889}\n",
            "  ‚Ä¢ Extracurricular_Activities: {'No': 3493, 'Yes': 1507}\n",
            "  ‚Ä¢ Internet_Access_at_Home: {'Yes': 4485, 'No': 515}\n",
            "  ‚Ä¢ Parent_Education_Level: {'PhD': 820, \"Bachelor's\": 810, 'High School': 796}\n",
            "  ‚Ä¢ Family_Income_Level: {'Low': 1983, 'Medium': 1973, 'High': 1044}\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "Summarize: CSV File Summary. Total Rows: 5000. Total Columns: 23. Column Names: Student_ID, First_Name, Last_Name,. Email, Gender, Age, Department, Attendance (%), Midterm_Score,. Final_Score, Assignments_Avg, Quizzes_Avg,. Participation_Score.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF FILE INPUT"
      ],
      "metadata": {
        "id": "v0i3asVj58kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Sl804G19Pw",
        "outputId": "df3a23a7-cf3b-43c8-ea5e-c90c2f304f3b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): /content/Data Science Interview.pdf\n",
            "‚úÖ Processing /content/Data Science Interview.pdf ...\n",
            "‚úÖ Successfully processed /content/Data Science Interview.pdf.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit):  WHAT IS A DATA INCIDENT IN HUMANITARIAN RESPONSE\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "WHAT IS A DATA INCIDENT IN HUMANITARIAN RESPONSE?\n",
            "In the humanitarian sector, data incidents are events involving the management of data that have caused\n",
            "harm or have the potential to cause harm to crisis affected populations, humanitarian organisations\n",
            "and their operations, and other individuals or groups.\n",
            "THE CENTRE FOR HUMANITARIAN DATA\n",
            "GUIDANCE NOTE SERIES\n",
            "DATA RESPONSIBILITY IN HUMANITARIAN ACTION\n",
            "DATA INCIDENT MANAGEMENT\n",
            "KEY TAKEAWAYS:\n",
            "‚Ä¢ Humanitarian data incidents are events involving the management of data that have caused\n",
            "harm or have the potential to cause harm to crisis-affected people, organizations and their\n",
            "operations, and other individuals or groups.\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "In the humanitarian sector, data incidents are events involving the management of data that have caused or have the potential to cause harm to crisis affected populations, humanitarian organisations and other individuals or groups. Humanitarian data incidents can be managed using the Humanitarian Data Note Series.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMAGE FILE INPUT\n"
      ],
      "metadata": {
        "id": "w5iwy4Eq6CNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izkIFESM2nOE",
        "outputId": "ae90af0e-a7cf-4054-c36c-582b7f0053c1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): /content/Screenshot 2024-07-12 232612.png\n",
            "‚úÖ Processing /content/Screenshot 2024-07-12 232612.png ...\n",
            "‚úÖ Successfully processed /content/Screenshot 2024-07-12 232612.png.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): list there components if presnt\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "Digital Marketing Types\n",
            "\n",
            "Content Marketing\n",
            "\n",
            "Social Media Marketing ce =| A SEO Search Engine Optimization\n",
            "\n",
            "E-mail Marketing a\n",
            "Affiliate Marketing ¬Æ\n",
            "\n",
            "Viral Marketing\n",
            "\n",
            "  \n",
            "\n",
            "j | Search Engine Marketing\n",
            "\n",
            "ro Influencer Marketing\n",
            "Digital Marketing Types\n",
            "\n",
            "Content Marketing\n",
            "\n",
            "Social Media Marketing ce =| A SEO Search Engine Optimization\n",
            "\n",
            "E-mail Marketing a\n",
            "Affiliate Marketing ¬Æ\n",
            "\n",
            "Viral Marketing\n",
            "\n",
            "  \n",
            "\n",
            "j | Search Engine Marketing\n",
            "\n",
            "ro Influencer Marketing\n",
            "Digital Marketing Types\n",
            "\n",
            "Content Marketing\n",
            "\n",
            "Social Media Marketing ce =| A SEO Search Engine Optimization\n",
            "\n",
            "E-mail Marketing a\n",
            "Affiliate Marketing ¬Æ\n",
            "\n",
            "Viral Marketing\n",
            "\n",
            "  \n",
            "\n",
            "j | Search Engine Marketing\n",
            "\n",
            "ro Influencer Marketing\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "Summarize: Digital Marketing Types. Digital Marketing types: Content Marketing, Social Media Marketing, Search Engine Marketing, Affiliate Marketing, Influencer Marketing, and Viral Marketing. Search Engine marketing types: Influencer marketing, Digital Marketing, E-mail Marketing.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIDEO FILE INPUT"
      ],
      "metadata": {
        "id": "tf4TpyJU6FWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EoS6ndk2oKM",
        "outputId": "727c6e53-6935-4c79-934c-804ebd1da2e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): /content/videoplayback.mp4\n",
            "‚úÖ Processing /content/videoplayback.mp4 ...\n",
            "MoviePy - Writing audio in temp_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:01<00:00, 118MiB/s]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Successfully processed /content/videoplayback.mp4.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): explain it\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "Their ability to retain information over long sequences makes them superior to simple RNNs and tasks requiring long-term context.\n",
            "Long short-term memory networks, or LSTMs, are a sophisticated type of RNN designed to address the vanishing gradient problem.\n",
            "Unlike simple RNNs, LSTMs incorporate a more complex cell structure that allows them to remember information for extended periods.\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "Long short-term memory networks, or LSTMs, are a sophisticated type of RNN designed to address the vanishing gradient problem. Their ability to retain information over long sequences makes them superior to simple RNNs and tasks requiring long-term context.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOCS INPUT FILE"
      ],
      "metadata": {
        "id": "lZTqRvJd6aMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HedQ7OM53Rfm",
        "outputId": "23922aa2-c115-4472-de48-584d46e7c73e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): /content/db.docx\n",
            "‚úÖ Processing /content/db.docx ...\n",
            "‚úÖ Successfully processed /content/db.docx.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): explain it\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "db.events.insert([\n",
            "  {\n",
            "    eventId: 1,\n",
            "    name: \"Tech Conference 2025\",\n",
            "    date: ISODate(\"2025-06-15\"),\n",
            "    location: \"chennai\",\n",
            "    attendees: 500,\n",
            "    organizer: \"Tech Corporate\",\n",
            "    ticketsAvailable: 150\n",
            "  },\n",
            "  {\n",
            "    eventId: 2,\n",
            "    name: \"Music Festival\",\n",
            "    date: ISODate(\"2025-07-20\"),\n",
            "    location: Coimbatore\",\n",
            "    attendees: 2000,\n",
            "    organizer: \"Vijay Antony\",\n",
            "    ticketsAvailable: 50\n",
            "  },\n",
            "  {\n",
            "    eventId: 3,\n",
            "    name: \"AI Workshop\",\n",
            "    date: ISODate(\"2025-11-10\"),\n",
            "    location: \"CIT\",\n",
            "    attendees: 100,\n",
            "    organizer: \"AIML Lab\",\n",
            "    ticketsAvailable: 20\n",
            "  },\n",
            "{\n",
            "    eventId: 4,\n",
            "    name: \"Dance Marathon\",\n",
            "    date: ISODate(\"2025-09-24\"),\n",
            "    location: \"PSG TECH\",\n",
            "    attendees: 100,\n",
            "    organizer: \"College Entrance\",\n",
            "    ticketsAvailable: 20\n",
            "  },\n",
            "{\n",
            "    eventId: 5,\n",
            "    name: \"Hackathon\",\n",
            "    date: ISODate(\"2025-01-03\"),\n",
            "    location: \"CIT\",\n",
            "    attendees: 100,\n",
            "    organizer: \"AIML Lab\",\n",
            "    ticketsAvailable: 20\n",
            "  }\n",
            "\n",
            "\n",
            "])\n",
            "db.events.insert([\n",
            "  {\n",
            "    eventId: 1,\n",
            "    name: \"Tech Conference 2025\",\n",
            "    date: ISODate(\"2025-06-15\"),\n",
            "    location: \"chennai\",\n",
            "    attendees: 500,\n",
            "    organizer: \"Tech Corporate\",\n",
            "    ticketsAvailable: 150\n",
            "  },\n",
            "  {\n",
            "    eventId: 2,\n",
            "    name: \"Music Festival\",\n",
            "    date: ISODate(\"2025-07-20\"),\n",
            "    location: Coimbatore\",\n",
            "    attendees: 2000,\n",
            "    organizer: \"Vijay Antony\",\n",
            "    ticketsAvailable: 50\n",
            "  },\n",
            "  {\n",
            "    eventId: 3,\n",
            "    name: \"AI Workshop\",\n",
            "    date: ISODate(\"2025-11-10\"),\n",
            "    location: \"CIT\",\n",
            "    attendees: 100,\n",
            "    organizer: \"AIML Lab\",\n",
            "    ticketsAvailable: 20\n",
            "  },\n",
            "{\n",
            "    eventId: 4,\n",
            "    name: \"Dance Marathon\",\n",
            "    date: ISODate(\"2025-09-24\"),\n",
            "    location: \"PSG TECH\",\n",
            "    attendees: 100,\n",
            "    organizer: \"College Entrance\",\n",
            "    ticketsAvailable: 20\n",
            "  },\n",
            "{\n",
            "    eventId: 5,\n",
            "    name: \"Hackathon\",\n",
            "    date: ISODate(\"2025-01-03\"),\n",
            "    location: \"CIT\",\n",
            "    attendees: 100,\n",
            "    organizer: \"AIML Lab\",\n",
            "    ticketsAvailable: 20\n",
            "  }\n",
            "\n",
            "\n",
            "])\n",
            "db.events.insert([\n",
            "  {\n",
            "    eventId: 1,\n",
            "    name: \"Tech Conference 2025\",\n",
            "    date: ISODate(\"2025-06-15\"),\n",
            "    location: \"chennai\",\n",
            "    attendees: 500,\n",
            "    organizer: \"Tech Corporate\",\n",
            "    ticketsAvailable: 150\n",
            "  },\n",
            "  {\n",
            "    eventId: 2,\n",
            "    name: \"Music Festival\",\n",
            "    date: ISODate(\"2025-07-20\"),\n",
            "    location: Coimbatore\",\n",
            "    attendees: 2000,\n",
            "    organizer: \"Vijay Antony\",\n",
            "    ticketsAvailable: 50\n",
            "  },\n",
            "  {\n",
            "    eventId: 3,\n",
            "    name: \"AI Workshop\",\n",
            "    date: ISODate(\"2025-11-10\"),\n",
            "    location: \"CIT\",\n",
            "    attendees: 100,\n",
            "    organizer: \"AIML Lab\",\n",
            "    ticketsAvailable: 20\n",
            "  },\n",
            "{\n",
            "    eventId: 4,\n",
            "    name: \"Dance Marathon\",\n",
            "    date: ISODate(\"2025-09-24\"),\n",
            "    location: \"PSG TECH\",\n",
            "    attendees: 100,\n",
            "    organizer: \"College Entrance\",\n",
            "    ticketsAvailable: 20\n",
            "  },\n",
            "{\n",
            "    eventId: 5,\n",
            "    name: \"Hackathon\",\n",
            "    date: ISODate(\"2025-01-03\"),\n",
            "    location: \"CIT\",\n",
            "    attendees: 100,\n",
            "    organizer: \"AIML Lab\",\n",
            "    ticketsAvailable: 20\n",
            "  }\n",
            "\n",
            "\n",
            "])\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "Summarize: db.events.insert('Tech Conference 2025', 'AI Workshop', 'Music Festival', 'Hackathon', 'Dance Marathon', 'College Entrance', 'Event', 'Conference'), 'EventId', 'Date', 'Time', 'Location', 'Ticket', 'Number', 'Price', 'Tickets', 'Amount', 'Total', 'Cost', 'Length', 'Prize', 'Value' & 'Price'\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "URL AS INPUT"
      ],
      "metadata": {
        "id": "i-5ZdUnz6eQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhrhkq7K3oVe",
        "outputId": "02cafe03-06c3-4aa2-9083-35ce07c24b6d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): https://www.geeksforgeeks.org/machine-learning-algorithms/\n",
            "‚úÖ Processing https://www.geeksforgeeks.org/machine-learning-algorithms/ ...\n",
            "‚úÖ Successfully processed https://www.geeksforgeeks.org/machine-learning-algorithms/.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): describe\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "For in-depth understanding : What is Ensemble Learning?\n",
            "For in-depth understanding : Supervised multi-layer perceptron model ‚Äì  What is perceptron?\n",
            "These methods use a model of the environment to predict outcomes and help the agent plan actions by simulating potential results.\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "These methods use a model of the environment to predict outcomes and help the agent plan actions by simulating potential results. These methods are called Ensemble Learning and Supervised multi-layer perceptron models. For in-depth understanding, read: What is Ensemble learning and what is perceptron?\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): perceptron means\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "For in-depth understanding : Supervised multi-layer perceptron model ‚Äì  What is perceptron?\n",
            ": Gradient Boosting in ML \n",
            "For more ensemble learning and gradient boosting approaches, explore:\n",
            "Neural Networks, including Multilayer Perceptrons (MLPs), are considered part of supervised machine learning algorithms¬†as they require labeled data to train and learn the relationship between input and desired output; network learns to minimize the error using backpropagation algorithm to adjust weights during training.\n",
            "It identifies patterns based solely on the frequency of item occurrences and co-occurrences in the dataset.\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "Neural Networks, including Multilayer Perceptrons (MLPs), are considered part of supervised machine learning algorithms. Network learns to minimize the error using backpropagation algorithm to adjust weights during training. It identifies patterns based solely on the frequency of item occurrences and co-occurrences.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUTUBE VIDEO LINK INPUT WITHOUT SUBTITLE"
      ],
      "metadata": {
        "id": "4onqfzCO6hac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h61SMWy4EbQ",
        "outputId": "76ef4393-2326-4f57-92b1-8d4ad9e07302"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): https://www.youtube.com/shorts/CJ_ZAmjyiQg\n",
            "‚úÖ Processing https://www.youtube.com/shorts/CJ_ZAmjyiQg ...\n",
            "‚úÖ Successfully processed https://www.youtube.com/shorts/CJ_ZAmjyiQg.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): explain it\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "This is most likely caused by:\n",
            "\n",
            "You provided an invalid video id.\n",
            "Error extracting subtitles from YouTube: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=https://www.youtube.com/shorts/CJ_ZAmjyiQg!\n",
            "Do NOT run: `YouTubeTranscriptApi.get_transcript(\"https://www.youtube.com/watch?v=1234\")`\n",
            "Instead run: `YouTubeTranscriptApi.get_transcript(\"1234\")`\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues.\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "Error extracting subtitles from YouTube. Could not retrieve a transcript for the video. This is most likely caused by: You provided an invalid video id. Do NOT run: `YouTubeTranscriptApi.get_transcript(\"https://www.youtube.com/watch?v=1234\")\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUTUBE VIDEO WITH SUBTITLE INPUT"
      ],
      "metadata": {
        "id": "cedGf0Bf6pOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIm_qq9k4eWc",
        "outputId": "2f0ff539-3ef7-4746-e24e-ce4018b06bd8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): https://www.youtube.com/watch?v=VJHWOO2Ix_8&t=330s\n",
            "‚úÖ Processing https://www.youtube.com/watch?v=VJHWOO2Ix_8&t=330s ...\n",
            "‚úÖ Successfully processed https://www.youtube.com/watch?v=VJHWOO2Ix_8&t=330s.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): explain it\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "there are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory now in this video we will discuss what is lstm why do we need it and what is the problem with the recurrent neural network now this is the model which is a bit hard to understand and especially when you are looking at it or understanding it for the first time it does not quite make sense what is going on and same thing happened to me when i learned this concept for the first time i did not understand much so after going through a lot of resources i have wrapped everything into this single video which will not only give you the mathematical details behind lstm but also the intuition behind it which is very important to understand what is going on and i just forgot to tell you that at the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz and i hope that after watching this video you do not need to go anywhere else to understand lsdn and after watching this video if you understand lstm properly then do hit the like button show your support because it takes a lot of time and energy to make such kind of videos and you also know why should i repeat it again hit the red subscribe button also hit the bell icon so that you get notified every time i upload new video hit the red subscribe button and okay okay i know what you are saying stop talking joy start the video so let's get started let's say if we are making a grammar checker which checks if the given sentence is grammatically correct or not so this is a website called quiltbot which provides grammar checker and in this sentence if we read this sentence it says mr watson goes on a business trip every wednesday and one day during his summer break her wife was very angry now clearly this should be his not her and that's what the quillbot is suggesting us now this means that if we use a recurrent neural network for creating a grammar checker then this recurrent neural network should suggest us that here we should have his and not her because here the subject is mr watson which is a male but practically recurrent neural network are not able to do so let's see why let's say this is our recurrent neural network and we pass this sentence to it to check if it is grammatically correct or not now the individual words will go as an input to this recurrent neural network but when we pass this long sentence to the recurrent neural network it actually will fail to identify this as an error let's see why what happens in a recurring neural network is that these a is or which are activations or hidden state is passed from the previous timestamp to the next time stem till the end of the sentences and during each time stamp the value of the a are updated so by the time it reaches to this time stamp the value of the a has been updated so many times that it has almost lost the information that was present when it saw the word mr watson first that is why the problem with the recur neural network is that whenever we pass a long sentence it loses the information that was present very earlier in the sentence it remembers the information from the last few of the words but it fails to remember the words or the information that appeared long before now another way to prove this is by using the back propagation if you have watched my previous video on track propagation then you would know that we derived this del by del w with the help of this equation where which is the summation from i equal to 1 till all the time stem and here we have derivative of o which is the output with respect to the activation a and this activation a will be for all the time stem 1 till t so when we expand this we get this as an equation now this part represents the derivative of output o at the time stem t with respect to the activation or the hidden state at the timestamp 1 which means that when we are back propagating from the timestamp t till the time stem 1 we have to multiply lot of gradients to reach to the time stem 1 the value of the gradients are between 0 to 1 which means that the gradient value is a small number and whenever we multiply a lot of small numbers the net result becomes almost close to zero so the net result of this term will be almost equal to zero which means that while updating the weight which is actual training of our model the words that appear at the earlier stage has almost no say and this problem is known as vanishing gradient problem the word vanishing gradients means that the gradient value becomes so less that it becomes almost equal to zero and that is why we need another model that has a long term memory along with the short term memory in the rnn the value of the a's are updated at every time stem so they quickly lose the information that was provided back then and that is why they have short term memory but in lstm we have a long term memory as well as a short term memory and that is why the name is long short term memory here in lstm we have two states instead of just one state as we had in rnn the first state c here is called as a memory cell state and the second state here is called as hidden state this memory cell state will be responsible for retaining knowledge into its memory so that we can use it later on if we need it and this hidden state acts as the hidden state that we had in the recurrent neural network the long term memory will be able to retain the context or the knowledge that was provided along back and the main reason why the lstm is able to do that is because it used a concept of gates now there are three kinds of gates in lstm one is forget gate another is input gate and third is output gate now the concept of gate is that a gate allows certain information to pass through and it restricts the other information this means that a forget gate will be responsible for allowing our model to forget some information or retain some information so if it does not allow some context or some knowledge to forget then that knowledge will be retained throughout many time stem we will understand these in detail in just a while and the input gate will be responsible for adding new information so whenever our model sees new information provided by input x the input gate will allow some information to be added to our network while it won't allow some information to be added so it will only allow useful information to add to our model and the output gate will be responsible for letting us know what to produce as an output let's look at them in detail so let's look at every component one by one we will first start with looking at our memory cell state ct in the equation of ct it has two components one have forget gate and other have input gate let's see first what is the role of forget gate here now if you see here the forget gate is multiplied with the c from the previous time stem and this is an element-wise multiplication which means that the shape of ft will be same as the shape of c t minus 1 and the values inside the matrix f t will be 0 or 1 so the values of the forget gate matrix will be multiplied with the values with the c t minus 1 and as it is the element wise multiplication which means that this value will be multiplied with this this will be multiplied with this and so on and that is why let's say when we have one here in the forget gate it will retain that information or it will retain that value and whenever we have zero in the forget gate then it will eliminate that information or that value and that is why it acts as a gate 1 means allowed to pass through while 0 means discard that information and that is why it is called forget git because it can forget some information and now let's see why do we want to forget some information let's say we have this long sentence here whenever a model sees mr watson it will retain that information or this context into the cell state c1 now c will be a matrix right so some of its value will retain the information that we have mr watson here and now the forget gate will not allow the information here to change that is why the information will retain throughout in the simple recurrent neural network these values would have updated quickly as our model sees the next word but it does not happen in lstm and that is why let's say when it reaches at this world her it recognized that here it should be his wife and not her wife because the sea has still stored mr watson in its memory and let's say we encounter some other word miss mary then now the forget gate will forget the information of mr watson but it will add the information of miss mary and that is why we have this input gate i t here and as it has forgotten that we had mr watson and it has a remake and as it has retained that we have miss mary now then our model can decide that here we should have herself and not himself because miss mary is a female now we will understand the second term of our ct which is this part this has an input gate i t and this c tilde t is known as candidate value the equation for the candidate value is given by this now the equation of the candidate value is almost the same as the equation of a that we had in recurrent neural network in the simple recurrent neural network now if you remember the equation of the a in simple recurrent neural network was given by this which can also be represented as this here what i have done is that i have combined the weight matrix into wa and i have concatenated the 80 minus 1 and x t this means that the weight matrix w a here is a combination of w a a and w a x and a t minus 1 is concatenated vertically with x t and thus when we multiply this we will get w a a 80 minus 1 and this equation back so what i wanted to say is that these two equations are the same and here this represent that a t minus 1 is concatenated with x t and now we have the w a matrix which has both w a a and w a x this means that the s equation of a t is similar to the our candidate value here this candidate value will be responsible for adding new information and as its name suggests that it's a candidate value which means that potential new information that we can add and that potential new information will be filtered by this input gate this means that the input gate i t will also have the 0 and 1 as its values and thus it will filter out what new information it wants to add let us understand this with the help of an example the statement says that mr watson goes on a business trip every wednesday and one day during a summer break harvard was very angry and tomorrow he was need to go on a trip again now clearly this should be his and this should be will because here we have tomorrow the proper statement would have been and tomorrow he will need to go on a trip again so whenever our model reaches this word tomorrow it also needs to add that information along with whatever information it had before so this is an example where we do not need to forget anything but we are just adding new information while retaining the useful information that we had before thus this model has the capacity to retain the old information for a long time along with adding new information now we discussed that the gate stores 0 and 1 as its value but let's see the equation of these input gate and the forget gate as well the equation of the forget gate and the input gate is given by this you will notice that this component is almost same in all of these the only difference is of the weights here we have wc here we have wf here we have wi and similarly for the biases as well and here we are applying sigmoid activation function now as it is a sigmoid activation function the values of the ft will be between 0 and 1 only so most of its value will be either close to 0 or either close to 1. so if the value is close to 0 it means that we want to forget that information and if the value is close to 1 that means that we want to retain that information and that is why they are able to act as gates now you might ask me that hey jay how does these gates get to know which information to forget and which information to add and the answer lies in the weight matrices here notice that we will train this model and after training the values of these weights will be updated in such a way that it will build a certain level of understanding which means that after training our model will develop an understanding of which information is useful and which information is irrelevant and this understanding is built after looking at thousands and thousands of data or sentences now these equations diagrammatically look something like this here we will provide the input word which will be provided at every time stem here we have hidden state and here we have cell state now this box here states that it is a neural network a neural network means that the hidden state and the input x will be multiplied with the weights and it will be passed on to some activation function and and then we will have this forget gate which will be multiplied with ct minus 1. so we have forget gate multiplied by ct minus 1 and an input gate multiplied by candidate value which is obtained from here now we have looked at two gates now it's time to look at our third gate which is the output gate the value of the hidden state ht will be computed by multiplying the output gate and by passing the tan h over c t and this hidden state will be provided as an output to make the prediction the equation of the output gate is similar to the equation of the forget and the input gate the only difference is that we have the weight w0 and the bias be 0. now this output gate will filter out what information to be predicted or given as the output by what information to be not given in the output now i know we discussed a lot of things here so a quick revision will be very very useful here so let us revise everything that we have learned so far lstm will have two states one is the cell state and the other is the hidden state the cell state will act as a long term memory which will be able to retain some information through long time and with the help of the hidden state and the input will also keep on adding new information as we encounter the cell state ct has two components one is this and the other is this the first component has forget gate and it is multiplied with the cell state of the previous timestamp this means that the forget gate will help us to decide what information to forget from the previous time step and what information to retain if the forget gate has not allowed to forget some information that means that we have still retained that information through a very very long time now this second component has input gate and the candidate value the candidate value tells us what new information can be potentially added and then this input gate will then decide if it wants to add that value or not the equations for the gates are given by this and as we have sigmoid here the values of these gates will be mostly either close to zero or either close to 1 and finally the hidden state will be updated with the help of this equation where the output gate is multiplied with the 10 h of c t this was regarding the one cell of lstm and this is how lstm look like throughout the time let's say this is our timestamp t equal to t1 this is t equal to 2 and this is t equal to t let's say then these cell state and the hidden state values will be passed on to the further time stem and this output prediction is made using this hidden state h t notice this is a neural network which means that the equation of ot will be given by applying soft max over w y multiplied by h t plus b y so this was everything regarding lstm i know it was a long video but i hope you learned something from it make sure you give the quiz which is down in the description box it will help you develop more understanding of this concept and remember what you have learned and if you found this video valuable then please do hit the like button it took me a lot of time to make this video share your support so that i keep on making such videos and up till then i will see you in the next video\n",
            "there are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory now in this video we will discuss what is lstm why do we need it and what is the problem with the recurrent neural network now this is the model which is a bit hard to understand and especially when you are looking at it or understanding it for the first time it does not quite make sense what is going on and same thing happened to me when i learned this concept for the first time i did not understand much so after going through a lot of resources i have wrapped everything into this single video which will not only give you the mathematical details behind lstm but also the intuition behind it which is very important to understand what is going on and i just forgot to tell you that at the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz and i hope that after watching this video you do not need to go anywhere else to understand lsdn and after watching this video if you understand lstm properly then do hit the like button show your support because it takes a lot of time and energy to make such kind of videos and you also know why should i repeat it again hit the red subscribe button also hit the bell icon so that you get notified every time i upload new video hit the red subscribe button and okay okay i know what you are saying stop talking joy start the video so let's get started let's say if we are making a grammar checker which checks if the given sentence is grammatically correct or not so this is a website called quiltbot which provides grammar checker and in this sentence if we read this sentence it says mr watson goes on a business trip every wednesday and one day during his summer break her wife was very angry now clearly this should be his not her and that's what the quillbot is suggesting us now this means that if we use a recurrent neural network for creating a grammar checker then this recurrent neural network should suggest us that here we should have his and not her because here the subject is mr watson which is a male but practically recurrent neural network are not able to do so let's see why let's say this is our recurrent neural network and we pass this sentence to it to check if it is grammatically correct or not now the individual words will go as an input to this recurrent neural network but when we pass this long sentence to the recurrent neural network it actually will fail to identify this as an error let's see why what happens in a recurring neural network is that these a is or which are activations or hidden state is passed from the previous timestamp to the next time stem till the end of the sentences and during each time stamp the value of the a are updated so by the time it reaches to this time stamp the value of the a has been updated so many times that it has almost lost the information that was present when it saw the word mr watson first that is why the problem with the recur neural network is that whenever we pass a long sentence it loses the information that was present very earlier in the sentence it remembers the information from the last few of the words but it fails to remember the words or the information that appeared long before now another way to prove this is by using the back propagation if you have watched my previous video on track propagation then you would know that we derived this del by del w with the help of this equation where which is the summation from i equal to 1 till all the time stem and here we have derivative of o which is the output with respect to the activation a and this activation a will be for all the time stem 1 till t so when we expand this we get this as an equation now this part represents the derivative of output o at the time stem t with respect to the activation or the hidden state at the timestamp 1 which means that when we are back propagating from the timestamp t till the time stem 1 we have to multiply lot of gradients to reach to the time stem 1 the value of the gradients are between 0 to 1 which means that the gradient value is a small number and whenever we multiply a lot of small numbers the net result becomes almost close to zero so the net result of this term will be almost equal to zero which means that while updating the weight which is actual training of our model the words that appear at the earlier stage has almost no say and this problem is known as vanishing gradient problem the word vanishing gradients means that the gradient value becomes so less that it becomes almost equal to zero and that is why we need another model that has a long term memory along with the short term memory in the rnn the value of the a's are updated at every time stem so they quickly lose the information that was provided back then and that is why they have short term memory but in lstm we have a long term memory as well as a short term memory and that is why the name is long short term memory here in lstm we have two states instead of just one state as we had in rnn the first state c here is called as a memory cell state and the second state here is called as hidden state this memory cell state will be responsible for retaining knowledge into its memory so that we can use it later on if we need it and this hidden state acts as the hidden state that we had in the recurrent neural network the long term memory will be able to retain the context or the knowledge that was provided along back and the main reason why the lstm is able to do that is because it used a concept of gates now there are three kinds of gates in lstm one is forget gate another is input gate and third is output gate now the concept of gate is that a gate allows certain information to pass through and it restricts the other information this means that a forget gate will be responsible for allowing our model to forget some information or retain some information so if it does not allow some context or some knowledge to forget then that knowledge will be retained throughout many time stem we will understand these in detail in just a while and the input gate will be responsible for adding new information so whenever our model sees new information provided by input x the input gate will allow some information to be added to our network while it won't allow some information to be added so it will only allow useful information to add to our model and the output gate will be responsible for letting us know what to produce as an output let's look at them in detail so let's look at every component one by one we will first start with looking at our memory cell state ct in the equation of ct it has two components one have forget gate and other have input gate let's see first what is the role of forget gate here now if you see here the forget gate is multiplied with the c from the previous time stem and this is an element-wise multiplication which means that the shape of ft will be same as the shape of c t minus 1 and the values inside the matrix f t will be 0 or 1 so the values of the forget gate matrix will be multiplied with the values with the c t minus 1 and as it is the element wise multiplication which means that this value will be multiplied with this this will be multiplied with this and so on and that is why let's say when we have one here in the forget gate it will retain that information or it will retain that value and whenever we have zero in the forget gate then it will eliminate that information or that value and that is why it acts as a gate 1 means allowed to pass through while 0 means discard that information and that is why it is called forget git because it can forget some information and now let's see why do we want to forget some information let's say we have this long sentence here whenever a model sees mr watson it will retain that information or this context into the cell state c1 now c will be a matrix right so some of its value will retain the information that we have mr watson here and now the forget gate will not allow the information here to change that is why the information will retain throughout in the simple recurrent neural network these values would have updated quickly as our model sees the next word but it does not happen in lstm and that is why let's say when it reaches at this world her it recognized that here it should be his wife and not her wife because the sea has still stored mr watson in its memory and let's say we encounter some other word miss mary then now the forget gate will forget the information of mr watson but it will add the information of miss mary and that is why we have this input gate i t here and as it has forgotten that we had mr watson and it has a remake and as it has retained that we have miss mary now then our model can decide that here we should have herself and not himself because miss mary is a female now we will understand the second term of our ct which is this part this has an input gate i t and this c tilde t is known as candidate value the equation for the candidate value is given by this now the equation of the candidate value is almost the same as the equation of a that we had in recurrent neural network in the simple recurrent neural network now if you remember the equation of the a in simple recurrent neural network was given by this which can also be represented as this here what i have done is that i have combined the weight matrix into wa and i have concatenated the 80 minus 1 and x t this means that the weight matrix w a here is a combination of w a a and w a x and a t minus 1 is concatenated vertically with x t and thus when we multiply this we will get w a a 80 minus 1 and this equation back so what i wanted to say is that these two equations are the same and here this represent that a t minus 1 is concatenated with x t and now we have the w a matrix which has both w a a and w a x this means that the s equation of a t is similar to the our candidate value here this candidate value will be responsible for adding new information and as its name suggests that it's a candidate value which means that potential new information that we can add and that potential new information will be filtered by this input gate this means that the input gate i t will also have the 0 and 1 as its values and thus it will filter out what new information it wants to add let us understand this with the help of an example the statement says that mr watson goes on a business trip every wednesday and one day during a summer break harvard was very angry and tomorrow he was need to go on a trip again now clearly this should be his and this should be will because here we have tomorrow the proper statement would have been and tomorrow he will need to go on a trip again so whenever our model reaches this word tomorrow it also needs to add that information along with whatever information it had before so this is an example where we do not need to forget anything but we are just adding new information while retaining the useful information that we had before thus this model has the capacity to retain the old information for a long time along with adding new information now we discussed that the gate stores 0 and 1 as its value but let's see the equation of these input gate and the forget gate as well the equation of the forget gate and the input gate is given by this you will notice that this component is almost same in all of these the only difference is of the weights here we have wc here we have wf here we have wi and similarly for the biases as well and here we are applying sigmoid activation function now as it is a sigmoid activation function the values of the ft will be between 0 and 1 only so most of its value will be either close to 0 or either close to 1. so if the value is close to 0 it means that we want to forget that information and if the value is close to 1 that means that we want to retain that information and that is why they are able to act as gates now you might ask me that hey jay how does these gates get to know which information to forget and which information to add and the answer lies in the weight matrices here notice that we will train this model and after training the values of these weights will be updated in such a way that it will build a certain level of understanding which means that after training our model will develop an understanding of which information is useful and which information is irrelevant and this understanding is built after looking at thousands and thousands of data or sentences now these equations diagrammatically look something like this here we will provide the input word which will be provided at every time stem here we have hidden state and here we have cell state now this box here states that it is a neural network a neural network means that the hidden state and the input x will be multiplied with the weights and it will be passed on to some activation function and and then we will have this forget gate which will be multiplied with ct minus 1. so we have forget gate multiplied by ct minus 1 and an input gate multiplied by candidate value which is obtained from here now we have looked at two gates now it's time to look at our third gate which is the output gate the value of the hidden state ht will be computed by multiplying the output gate and by passing the tan h over c t and this hidden state will be provided as an output to make the prediction the equation of the output gate is similar to the equation of the forget and the input gate the only difference is that we have the weight w0 and the bias be 0. now this output gate will filter out what information to be predicted or given as the output by what information to be not given in the output now i know we discussed a lot of things here so a quick revision will be very very useful here so let us revise everything that we have learned so far lstm will have two states one is the cell state and the other is the hidden state the cell state will act as a long term memory which will be able to retain some information through long time and with the help of the hidden state and the input will also keep on adding new information as we encounter the cell state ct has two components one is this and the other is this the first component has forget gate and it is multiplied with the cell state of the previous timestamp this means that the forget gate will help us to decide what information to forget from the previous time step and what information to retain if the forget gate has not allowed to forget some information that means that we have still retained that information through a very very long time now this second component has input gate and the candidate value the candidate value tells us what new information can be potentially added and then this input gate will then decide if it wants to add that value or not the equations for the gates are given by this and as we have sigmoid here the values of these gates will be mostly either close to zero or either close to 1 and finally the hidden state will be updated with the help of this equation where the output gate is multiplied with the 10 h of c t this was regarding the one cell of lstm and this is how lstm look like throughout the time let's say this is our timestamp t equal to t1 this is t equal to 2 and this is t equal to t let's say then these cell state and the hidden state values will be passed on to the further time stem and this output prediction is made using this hidden state h t notice this is a neural network which means that the equation of ot will be given by applying soft max over w y multiplied by h t plus b y so this was everything regarding lstm i know it was a long video but i hope you learned something from it make sure you give the quiz which is down in the description box it will help you develop more understanding of this concept and remember what you have learned and if you found this video valuable then please do hit the like button it took me a lot of time to make this video share your support so that i keep on making such videos and up till then i will see you in the next video\n",
            "there are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory now in this video we will discuss what is lstm why do we need it and what is the problem with the recurrent neural network now this is the model which is a bit hard to understand and especially when you are looking at it or understanding it for the first time it does not quite make sense what is going on and same thing happened to me when i learned this concept for the first time i did not understand much so after going through a lot of resources i have wrapped everything into this single video which will not only give you the mathematical details behind lstm but also the intuition behind it which is very important to understand what is going on and i just forgot to tell you that at the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz and i hope that after watching this video you do not need to go anywhere else to understand lsdn and after watching this video if you understand lstm properly then do hit the like button show your support because it takes a lot of time and energy to make such kind of videos and you also know why should i repeat it again hit the red subscribe button also hit the bell icon so that you get notified every time i upload new video hit the red subscribe button and okay okay i know what you are saying stop talking joy start the video so let's get started let's say if we are making a grammar checker which checks if the given sentence is grammatically correct or not so this is a website called quiltbot which provides grammar checker and in this sentence if we read this sentence it says mr watson goes on a business trip every wednesday and one day during his summer break her wife was very angry now clearly this should be his not her and that's what the quillbot is suggesting us now this means that if we use a recurrent neural network for creating a grammar checker then this recurrent neural network should suggest us that here we should have his and not her because here the subject is mr watson which is a male but practically recurrent neural network are not able to do so let's see why let's say this is our recurrent neural network and we pass this sentence to it to check if it is grammatically correct or not now the individual words will go as an input to this recurrent neural network but when we pass this long sentence to the recurrent neural network it actually will fail to identify this as an error let's see why what happens in a recurring neural network is that these a is or which are activations or hidden state is passed from the previous timestamp to the next time stem till the end of the sentences and during each time stamp the value of the a are updated so by the time it reaches to this time stamp the value of the a has been updated so many times that it has almost lost the information that was present when it saw the word mr watson first that is why the problem with the recur neural network is that whenever we pass a long sentence it loses the information that was present very earlier in the sentence it remembers the information from the last few of the words but it fails to remember the words or the information that appeared long before now another way to prove this is by using the back propagation if you have watched my previous video on track propagation then you would know that we derived this del by del w with the help of this equation where which is the summation from i equal to 1 till all the time stem and here we have derivative of o which is the output with respect to the activation a and this activation a will be for all the time stem 1 till t so when we expand this we get this as an equation now this part represents the derivative of output o at the time stem t with respect to the activation or the hidden state at the timestamp 1 which means that when we are back propagating from the timestamp t till the time stem 1 we have to multiply lot of gradients to reach to the time stem 1 the value of the gradients are between 0 to 1 which means that the gradient value is a small number and whenever we multiply a lot of small numbers the net result becomes almost close to zero so the net result of this term will be almost equal to zero which means that while updating the weight which is actual training of our model the words that appear at the earlier stage has almost no say and this problem is known as vanishing gradient problem the word vanishing gradients means that the gradient value becomes so less that it becomes almost equal to zero and that is why we need another model that has a long term memory along with the short term memory in the rnn the value of the a's are updated at every time stem so they quickly lose the information that was provided back then and that is why they have short term memory but in lstm we have a long term memory as well as a short term memory and that is why the name is long short term memory here in lstm we have two states instead of just one state as we had in rnn the first state c here is called as a memory cell state and the second state here is called as hidden state this memory cell state will be responsible for retaining knowledge into its memory so that we can use it later on if we need it and this hidden state acts as the hidden state that we had in the recurrent neural network the long term memory will be able to retain the context or the knowledge that was provided along back and the main reason why the lstm is able to do that is because it used a concept of gates now there are three kinds of gates in lstm one is forget gate another is input gate and third is output gate now the concept of gate is that a gate allows certain information to pass through and it restricts the other information this means that a forget gate will be responsible for allowing our model to forget some information or retain some information so if it does not allow some context or some knowledge to forget then that knowledge will be retained throughout many time stem we will understand these in detail in just a while and the input gate will be responsible for adding new information so whenever our model sees new information provided by input x the input gate will allow some information to be added to our network while it won't allow some information to be added so it will only allow useful information to add to our model and the output gate will be responsible for letting us know what to produce as an output let's look at them in detail so let's look at every component one by one we will first start with looking at our memory cell state ct in the equation of ct it has two components one have forget gate and other have input gate let's see first what is the role of forget gate here now if you see here the forget gate is multiplied with the c from the previous time stem and this is an element-wise multiplication which means that the shape of ft will be same as the shape of c t minus 1 and the values inside the matrix f t will be 0 or 1 so the values of the forget gate matrix will be multiplied with the values with the c t minus 1 and as it is the element wise multiplication which means that this value will be multiplied with this this will be multiplied with this and so on and that is why let's say when we have one here in the forget gate it will retain that information or it will retain that value and whenever we have zero in the forget gate then it will eliminate that information or that value and that is why it acts as a gate 1 means allowed to pass through while 0 means discard that information and that is why it is called forget git because it can forget some information and now let's see why do we want to forget some information let's say we have this long sentence here whenever a model sees mr watson it will retain that information or this context into the cell state c1 now c will be a matrix right so some of its value will retain the information that we have mr watson here and now the forget gate will not allow the information here to change that is why the information will retain throughout in the simple recurrent neural network these values would have updated quickly as our model sees the next word but it does not happen in lstm and that is why let's say when it reaches at this world her it recognized that here it should be his wife and not her wife because the sea has still stored mr watson in its memory and let's say we encounter some other word miss mary then now the forget gate will forget the information of mr watson but it will add the information of miss mary and that is why we have this input gate i t here and as it has forgotten that we had mr watson and it has a remake and as it has retained that we have miss mary now then our model can decide that here we should have herself and not himself because miss mary is a female now we will understand the second term of our ct which is this part this has an input gate i t and this c tilde t is known as candidate value the equation for the candidate value is given by this now the equation of the candidate value is almost the same as the equation of a that we had in recurrent neural network in the simple recurrent neural network now if you remember the equation of the a in simple recurrent neural network was given by this which can also be represented as this here what i have done is that i have combined the weight matrix into wa and i have concatenated the 80 minus 1 and x t this means that the weight matrix w a here is a combination of w a a and w a x and a t minus 1 is concatenated vertically with x t and thus when we multiply this we will get w a a 80 minus 1 and this equation back so what i wanted to say is that these two equations are the same and here this represent that a t minus 1 is concatenated with x t and now we have the w a matrix which has both w a a and w a x this means that the s equation of a t is similar to the our candidate value here this candidate value will be responsible for adding new information and as its name suggests that it's a candidate value which means that potential new information that we can add and that potential new information will be filtered by this input gate this means that the input gate i t will also have the 0 and 1 as its values and thus it will filter out what new information it wants to add let us understand this with the help of an example the statement says that mr watson goes on a business trip every wednesday and one day during a summer break harvard was very angry and tomorrow he was need to go on a trip again now clearly this should be his and this should be will because here we have tomorrow the proper statement would have been and tomorrow he will need to go on a trip again so whenever our model reaches this word tomorrow it also needs to add that information along with whatever information it had before so this is an example where we do not need to forget anything but we are just adding new information while retaining the useful information that we had before thus this model has the capacity to retain the old information for a long time along with adding new information now we discussed that the gate stores 0 and 1 as its value but let's see the equation of these input gate and the forget gate as well the equation of the forget gate and the input gate is given by this you will notice that this component is almost same in all of these the only difference is of the weights here we have wc here we have wf here we have wi and similarly for the biases as well and here we are applying sigmoid activation function now as it is a sigmoid activation function the values of the ft will be between 0 and 1 only so most of its value will be either close to 0 or either close to 1. so if the value is close to 0 it means that we want to forget that information and if the value is close to 1 that means that we want to retain that information and that is why they are able to act as gates now you might ask me that hey jay how does these gates get to know which information to forget and which information to add and the answer lies in the weight matrices here notice that we will train this model and after training the values of these weights will be updated in such a way that it will build a certain level of understanding which means that after training our model will develop an understanding of which information is useful and which information is irrelevant and this understanding is built after looking at thousands and thousands of data or sentences now these equations diagrammatically look something like this here we will provide the input word which will be provided at every time stem here we have hidden state and here we have cell state now this box here states that it is a neural network a neural network means that the hidden state and the input x will be multiplied with the weights and it will be passed on to some activation function and and then we will have this forget gate which will be multiplied with ct minus 1. so we have forget gate multiplied by ct minus 1 and an input gate multiplied by candidate value which is obtained from here now we have looked at two gates now it's time to look at our third gate which is the output gate the value of the hidden state ht will be computed by multiplying the output gate and by passing the tan h over c t and this hidden state will be provided as an output to make the prediction the equation of the output gate is similar to the equation of the forget and the input gate the only difference is that we have the weight w0 and the bias be 0. now this output gate will filter out what information to be predicted or given as the output by what information to be not given in the output now i know we discussed a lot of things here so a quick revision will be very very useful here so let us revise everything that we have learned so far lstm will have two states one is the cell state and the other is the hidden state the cell state will act as a long term memory which will be able to retain some information through long time and with the help of the hidden state and the input will also keep on adding new information as we encounter the cell state ct has two components one is this and the other is this the first component has forget gate and it is multiplied with the cell state of the previous timestamp this means that the forget gate will help us to decide what information to forget from the previous time step and what information to retain if the forget gate has not allowed to forget some information that means that we have still retained that information through a very very long time now this second component has input gate and the candidate value the candidate value tells us what new information can be potentially added and then this input gate will then decide if it wants to add that value or not the equations for the gates are given by this and as we have sigmoid here the values of these gates will be mostly either close to zero or either close to 1 and finally the hidden state will be updated with the help of this equation where the output gate is multiplied with the 10 h of c t this was regarding the one cell of lstm and this is how lstm look like throughout the time let's say this is our timestamp t equal to t1 this is t equal to 2 and this is t equal to t let's say then these cell state and the hidden state values will be passed on to the further time stem and this output prediction is made using this hidden state h t notice this is a neural network which means that the equation of ot will be given by applying soft max over w y multiplied by h t plus b y so this was everything regarding lstm i know it was a long video but i hope you learned something from it make sure you give the quiz which is down in the description box it will help you develop more understanding of this concept and remember what you have learned and if you found this video valuable then please do hit the like button it took me a lot of time to make this video share your support so that i keep on making such videos and up till then i will see you in the next video\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "There are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory. At the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz. After watching this video do hit the like button show your support because it takes a lot of time and energy to make such kind of videos.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): explain it\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "there are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory now in this video we will discuss what is lstm why do we need it and what is the problem with the recurrent neural network now this is the model which is a bit hard to understand and especially when you are looking at it or understanding it for the first time it does not quite make sense what is going on and same thing happened to me when i learned this concept for the first time i did not understand much so after going through a lot of resources i have wrapped everything into this single video which will not only give you the mathematical details behind lstm but also the intuition behind it which is very important to understand what is going on and i just forgot to tell you that at the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz and i hope that after watching this video you do not need to go anywhere else to understand lsdn and after watching this video if you understand lstm properly then do hit the like button show your support because it takes a lot of time and energy to make such kind of videos and you also know why should i repeat it again hit the red subscribe button also hit the bell icon so that you get notified every time i upload new video hit the red subscribe button and okay okay i know what you are saying stop talking joy start the video so let's get started let's say if we are making a grammar checker which checks if the given sentence is grammatically correct or not so this is a website called quiltbot which provides grammar checker and in this sentence if we read this sentence it says mr watson goes on a business trip every wednesday and one day during his summer break her wife was very angry now clearly this should be his not her and that's what the quillbot is suggesting us now this means that if we use a recurrent neural network for creating a grammar checker then this recurrent neural network should suggest us that here we should have his and not her because here the subject is mr watson which is a male but practically recurrent neural network are not able to do so let's see why let's say this is our recurrent neural network and we pass this sentence to it to check if it is grammatically correct or not now the individual words will go as an input to this recurrent neural network but when we pass this long sentence to the recurrent neural network it actually will fail to identify this as an error let's see why what happens in a recurring neural network is that these a is or which are activations or hidden state is passed from the previous timestamp to the next time stem till the end of the sentences and during each time stamp the value of the a are updated so by the time it reaches to this time stamp the value of the a has been updated so many times that it has almost lost the information that was present when it saw the word mr watson first that is why the problem with the recur neural network is that whenever we pass a long sentence it loses the information that was present very earlier in the sentence it remembers the information from the last few of the words but it fails to remember the words or the information that appeared long before now another way to prove this is by using the back propagation if you have watched my previous video on track propagation then you would know that we derived this del by del w with the help of this equation where which is the summation from i equal to 1 till all the time stem and here we have derivative of o which is the output with respect to the activation a and this activation a will be for all the time stem 1 till t so when we expand this we get this as an equation now this part represents the derivative of output o at the time stem t with respect to the activation or the hidden state at the timestamp 1 which means that when we are back propagating from the timestamp t till the time stem 1 we have to multiply lot of gradients to reach to the time stem 1 the value of the gradients are between 0 to 1 which means that the gradient value is a small number and whenever we multiply a lot of small numbers the net result becomes almost close to zero so the net result of this term will be almost equal to zero which means that while updating the weight which is actual training of our model the words that appear at the earlier stage has almost no say and this problem is known as vanishing gradient problem the word vanishing gradients means that the gradient value becomes so less that it becomes almost equal to zero and that is why we need another model that has a long term memory along with the short term memory in the rnn the value of the a's are updated at every time stem so they quickly lose the information that was provided back then and that is why they have short term memory but in lstm we have a long term memory as well as a short term memory and that is why the name is long short term memory here in lstm we have two states instead of just one state as we had in rnn the first state c here is called as a memory cell state and the second state here is called as hidden state this memory cell state will be responsible for retaining knowledge into its memory so that we can use it later on if we need it and this hidden state acts as the hidden state that we had in the recurrent neural network the long term memory will be able to retain the context or the knowledge that was provided along back and the main reason why the lstm is able to do that is because it used a concept of gates now there are three kinds of gates in lstm one is forget gate another is input gate and third is output gate now the concept of gate is that a gate allows certain information to pass through and it restricts the other information this means that a forget gate will be responsible for allowing our model to forget some information or retain some information so if it does not allow some context or some knowledge to forget then that knowledge will be retained throughout many time stem we will understand these in detail in just a while and the input gate will be responsible for adding new information so whenever our model sees new information provided by input x the input gate will allow some information to be added to our network while it won't allow some information to be added so it will only allow useful information to add to our model and the output gate will be responsible for letting us know what to produce as an output let's look at them in detail so let's look at every component one by one we will first start with looking at our memory cell state ct in the equation of ct it has two components one have forget gate and other have input gate let's see first what is the role of forget gate here now if you see here the forget gate is multiplied with the c from the previous time stem and this is an element-wise multiplication which means that the shape of ft will be same as the shape of c t minus 1 and the values inside the matrix f t will be 0 or 1 so the values of the forget gate matrix will be multiplied with the values with the c t minus 1 and as it is the element wise multiplication which means that this value will be multiplied with this this will be multiplied with this and so on and that is why let's say when we have one here in the forget gate it will retain that information or it will retain that value and whenever we have zero in the forget gate then it will eliminate that information or that value and that is why it acts as a gate 1 means allowed to pass through while 0 means discard that information and that is why it is called forget git because it can forget some information and now let's see why do we want to forget some information let's say we have this long sentence here whenever a model sees mr watson it will retain that information or this context into the cell state c1 now c will be a matrix right so some of its value will retain the information that we have mr watson here and now the forget gate will not allow the information here to change that is why the information will retain throughout in the simple recurrent neural network these values would have updated quickly as our model sees the next word but it does not happen in lstm and that is why let's say when it reaches at this world her it recognized that here it should be his wife and not her wife because the sea has still stored mr watson in its memory and let's say we encounter some other word miss mary then now the forget gate will forget the information of mr watson but it will add the information of miss mary and that is why we have this input gate i t here and as it has forgotten that we had mr watson and it has a remake and as it has retained that we have miss mary now then our model can decide that here we should have herself and not himself because miss mary is a female now we will understand the second term of our ct which is this part this has an input gate i t and this c tilde t is known as candidate value the equation for the candidate value is given by this now the equation of the candidate value is almost the same as the equation of a that we had in recurrent neural network in the simple recurrent neural network now if you remember the equation of the a in simple recurrent neural network was given by this which can also be represented as this here what i have done is that i have combined the weight matrix into wa and i have concatenated the 80 minus 1 and x t this means that the weight matrix w a here is a combination of w a a and w a x and a t minus 1 is concatenated vertically with x t and thus when we multiply this we will get w a a 80 minus 1 and this equation back so what i wanted to say is that these two equations are the same and here this represent that a t minus 1 is concatenated with x t and now we have the w a matrix which has both w a a and w a x this means that the s equation of a t is similar to the our candidate value here this candidate value will be responsible for adding new information and as its name suggests that it's a candidate value which means that potential new information that we can add and that potential new information will be filtered by this input gate this means that the input gate i t will also have the 0 and 1 as its values and thus it will filter out what new information it wants to add let us understand this with the help of an example the statement says that mr watson goes on a business trip every wednesday and one day during a summer break harvard was very angry and tomorrow he was need to go on a trip again now clearly this should be his and this should be will because here we have tomorrow the proper statement would have been and tomorrow he will need to go on a trip again so whenever our model reaches this word tomorrow it also needs to add that information along with whatever information it had before so this is an example where we do not need to forget anything but we are just adding new information while retaining the useful information that we had before thus this model has the capacity to retain the old information for a long time along with adding new information now we discussed that the gate stores 0 and 1 as its value but let's see the equation of these input gate and the forget gate as well the equation of the forget gate and the input gate is given by this you will notice that this component is almost same in all of these the only difference is of the weights here we have wc here we have wf here we have wi and similarly for the biases as well and here we are applying sigmoid activation function now as it is a sigmoid activation function the values of the ft will be between 0 and 1 only so most of its value will be either close to 0 or either close to 1. so if the value is close to 0 it means that we want to forget that information and if the value is close to 1 that means that we want to retain that information and that is why they are able to act as gates now you might ask me that hey jay how does these gates get to know which information to forget and which information to add and the answer lies in the weight matrices here notice that we will train this model and after training the values of these weights will be updated in such a way that it will build a certain level of understanding which means that after training our model will develop an understanding of which information is useful and which information is irrelevant and this understanding is built after looking at thousands and thousands of data or sentences now these equations diagrammatically look something like this here we will provide the input word which will be provided at every time stem here we have hidden state and here we have cell state now this box here states that it is a neural network a neural network means that the hidden state and the input x will be multiplied with the weights and it will be passed on to some activation function and and then we will have this forget gate which will be multiplied with ct minus 1. so we have forget gate multiplied by ct minus 1 and an input gate multiplied by candidate value which is obtained from here now we have looked at two gates now it's time to look at our third gate which is the output gate the value of the hidden state ht will be computed by multiplying the output gate and by passing the tan h over c t and this hidden state will be provided as an output to make the prediction the equation of the output gate is similar to the equation of the forget and the input gate the only difference is that we have the weight w0 and the bias be 0. now this output gate will filter out what information to be predicted or given as the output by what information to be not given in the output now i know we discussed a lot of things here so a quick revision will be very very useful here so let us revise everything that we have learned so far lstm will have two states one is the cell state and the other is the hidden state the cell state will act as a long term memory which will be able to retain some information through long time and with the help of the hidden state and the input will also keep on adding new information as we encounter the cell state ct has two components one is this and the other is this the first component has forget gate and it is multiplied with the cell state of the previous timestamp this means that the forget gate will help us to decide what information to forget from the previous time step and what information to retain if the forget gate has not allowed to forget some information that means that we have still retained that information through a very very long time now this second component has input gate and the candidate value the candidate value tells us what new information can be potentially added and then this input gate will then decide if it wants to add that value or not the equations for the gates are given by this and as we have sigmoid here the values of these gates will be mostly either close to zero or either close to 1 and finally the hidden state will be updated with the help of this equation where the output gate is multiplied with the 10 h of c t this was regarding the one cell of lstm and this is how lstm look like throughout the time let's say this is our timestamp t equal to t1 this is t equal to 2 and this is t equal to t let's say then these cell state and the hidden state values will be passed on to the further time stem and this output prediction is made using this hidden state h t notice this is a neural network which means that the equation of ot will be given by applying soft max over w y multiplied by h t plus b y so this was everything regarding lstm i know it was a long video but i hope you learned something from it make sure you give the quiz which is down in the description box it will help you develop more understanding of this concept and remember what you have learned and if you found this video valuable then please do hit the like button it took me a lot of time to make this video share your support so that i keep on making such videos and up till then i will see you in the next video\n",
            "there are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory now in this video we will discuss what is lstm why do we need it and what is the problem with the recurrent neural network now this is the model which is a bit hard to understand and especially when you are looking at it or understanding it for the first time it does not quite make sense what is going on and same thing happened to me when i learned this concept for the first time i did not understand much so after going through a lot of resources i have wrapped everything into this single video which will not only give you the mathematical details behind lstm but also the intuition behind it which is very important to understand what is going on and i just forgot to tell you that at the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz and i hope that after watching this video you do not need to go anywhere else to understand lsdn and after watching this video if you understand lstm properly then do hit the like button show your support because it takes a lot of time and energy to make such kind of videos and you also know why should i repeat it again hit the red subscribe button also hit the bell icon so that you get notified every time i upload new video hit the red subscribe button and okay okay i know what you are saying stop talking joy start the video so let's get started let's say if we are making a grammar checker which checks if the given sentence is grammatically correct or not so this is a website called quiltbot which provides grammar checker and in this sentence if we read this sentence it says mr watson goes on a business trip every wednesday and one day during his summer break her wife was very angry now clearly this should be his not her and that's what the quillbot is suggesting us now this means that if we use a recurrent neural network for creating a grammar checker then this recurrent neural network should suggest us that here we should have his and not her because here the subject is mr watson which is a male but practically recurrent neural network are not able to do so let's see why let's say this is our recurrent neural network and we pass this sentence to it to check if it is grammatically correct or not now the individual words will go as an input to this recurrent neural network but when we pass this long sentence to the recurrent neural network it actually will fail to identify this as an error let's see why what happens in a recurring neural network is that these a is or which are activations or hidden state is passed from the previous timestamp to the next time stem till the end of the sentences and during each time stamp the value of the a are updated so by the time it reaches to this time stamp the value of the a has been updated so many times that it has almost lost the information that was present when it saw the word mr watson first that is why the problem with the recur neural network is that whenever we pass a long sentence it loses the information that was present very earlier in the sentence it remembers the information from the last few of the words but it fails to remember the words or the information that appeared long before now another way to prove this is by using the back propagation if you have watched my previous video on track propagation then you would know that we derived this del by del w with the help of this equation where which is the summation from i equal to 1 till all the time stem and here we have derivative of o which is the output with respect to the activation a and this activation a will be for all the time stem 1 till t so when we expand this we get this as an equation now this part represents the derivative of output o at the time stem t with respect to the activation or the hidden state at the timestamp 1 which means that when we are back propagating from the timestamp t till the time stem 1 we have to multiply lot of gradients to reach to the time stem 1 the value of the gradients are between 0 to 1 which means that the gradient value is a small number and whenever we multiply a lot of small numbers the net result becomes almost close to zero so the net result of this term will be almost equal to zero which means that while updating the weight which is actual training of our model the words that appear at the earlier stage has almost no say and this problem is known as vanishing gradient problem the word vanishing gradients means that the gradient value becomes so less that it becomes almost equal to zero and that is why we need another model that has a long term memory along with the short term memory in the rnn the value of the a's are updated at every time stem so they quickly lose the information that was provided back then and that is why they have short term memory but in lstm we have a long term memory as well as a short term memory and that is why the name is long short term memory here in lstm we have two states instead of just one state as we had in rnn the first state c here is called as a memory cell state and the second state here is called as hidden state this memory cell state will be responsible for retaining knowledge into its memory so that we can use it later on if we need it and this hidden state acts as the hidden state that we had in the recurrent neural network the long term memory will be able to retain the context or the knowledge that was provided along back and the main reason why the lstm is able to do that is because it used a concept of gates now there are three kinds of gates in lstm one is forget gate another is input gate and third is output gate now the concept of gate is that a gate allows certain information to pass through and it restricts the other information this means that a forget gate will be responsible for allowing our model to forget some information or retain some information so if it does not allow some context or some knowledge to forget then that knowledge will be retained throughout many time stem we will understand these in detail in just a while and the input gate will be responsible for adding new information so whenever our model sees new information provided by input x the input gate will allow some information to be added to our network while it won't allow some information to be added so it will only allow useful information to add to our model and the output gate will be responsible for letting us know what to produce as an output let's look at them in detail so let's look at every component one by one we will first start with looking at our memory cell state ct in the equation of ct it has two components one have forget gate and other have input gate let's see first what is the role of forget gate here now if you see here the forget gate is multiplied with the c from the previous time stem and this is an element-wise multiplication which means that the shape of ft will be same as the shape of c t minus 1 and the values inside the matrix f t will be 0 or 1 so the values of the forget gate matrix will be multiplied with the values with the c t minus 1 and as it is the element wise multiplication which means that this value will be multiplied with this this will be multiplied with this and so on and that is why let's say when we have one here in the forget gate it will retain that information or it will retain that value and whenever we have zero in the forget gate then it will eliminate that information or that value and that is why it acts as a gate 1 means allowed to pass through while 0 means discard that information and that is why it is called forget git because it can forget some information and now let's see why do we want to forget some information let's say we have this long sentence here whenever a model sees mr watson it will retain that information or this context into the cell state c1 now c will be a matrix right so some of its value will retain the information that we have mr watson here and now the forget gate will not allow the information here to change that is why the information will retain throughout in the simple recurrent neural network these values would have updated quickly as our model sees the next word but it does not happen in lstm and that is why let's say when it reaches at this world her it recognized that here it should be his wife and not her wife because the sea has still stored mr watson in its memory and let's say we encounter some other word miss mary then now the forget gate will forget the information of mr watson but it will add the information of miss mary and that is why we have this input gate i t here and as it has forgotten that we had mr watson and it has a remake and as it has retained that we have miss mary now then our model can decide that here we should have herself and not himself because miss mary is a female now we will understand the second term of our ct which is this part this has an input gate i t and this c tilde t is known as candidate value the equation for the candidate value is given by this now the equation of the candidate value is almost the same as the equation of a that we had in recurrent neural network in the simple recurrent neural network now if you remember the equation of the a in simple recurrent neural network was given by this which can also be represented as this here what i have done is that i have combined the weight matrix into wa and i have concatenated the 80 minus 1 and x t this means that the weight matrix w a here is a combination of w a a and w a x and a t minus 1 is concatenated vertically with x t and thus when we multiply this we will get w a a 80 minus 1 and this equation back so what i wanted to say is that these two equations are the same and here this represent that a t minus 1 is concatenated with x t and now we have the w a matrix which has both w a a and w a x this means that the s equation of a t is similar to the our candidate value here this candidate value will be responsible for adding new information and as its name suggests that it's a candidate value which means that potential new information that we can add and that potential new information will be filtered by this input gate this means that the input gate i t will also have the 0 and 1 as its values and thus it will filter out what new information it wants to add let us understand this with the help of an example the statement says that mr watson goes on a business trip every wednesday and one day during a summer break harvard was very angry and tomorrow he was need to go on a trip again now clearly this should be his and this should be will because here we have tomorrow the proper statement would have been and tomorrow he will need to go on a trip again so whenever our model reaches this word tomorrow it also needs to add that information along with whatever information it had before so this is an example where we do not need to forget anything but we are just adding new information while retaining the useful information that we had before thus this model has the capacity to retain the old information for a long time along with adding new information now we discussed that the gate stores 0 and 1 as its value but let's see the equation of these input gate and the forget gate as well the equation of the forget gate and the input gate is given by this you will notice that this component is almost same in all of these the only difference is of the weights here we have wc here we have wf here we have wi and similarly for the biases as well and here we are applying sigmoid activation function now as it is a sigmoid activation function the values of the ft will be between 0 and 1 only so most of its value will be either close to 0 or either close to 1. so if the value is close to 0 it means that we want to forget that information and if the value is close to 1 that means that we want to retain that information and that is why they are able to act as gates now you might ask me that hey jay how does these gates get to know which information to forget and which information to add and the answer lies in the weight matrices here notice that we will train this model and after training the values of these weights will be updated in such a way that it will build a certain level of understanding which means that after training our model will develop an understanding of which information is useful and which information is irrelevant and this understanding is built after looking at thousands and thousands of data or sentences now these equations diagrammatically look something like this here we will provide the input word which will be provided at every time stem here we have hidden state and here we have cell state now this box here states that it is a neural network a neural network means that the hidden state and the input x will be multiplied with the weights and it will be passed on to some activation function and and then we will have this forget gate which will be multiplied with ct minus 1. so we have forget gate multiplied by ct minus 1 and an input gate multiplied by candidate value which is obtained from here now we have looked at two gates now it's time to look at our third gate which is the output gate the value of the hidden state ht will be computed by multiplying the output gate and by passing the tan h over c t and this hidden state will be provided as an output to make the prediction the equation of the output gate is similar to the equation of the forget and the input gate the only difference is that we have the weight w0 and the bias be 0. now this output gate will filter out what information to be predicted or given as the output by what information to be not given in the output now i know we discussed a lot of things here so a quick revision will be very very useful here so let us revise everything that we have learned so far lstm will have two states one is the cell state and the other is the hidden state the cell state will act as a long term memory which will be able to retain some information through long time and with the help of the hidden state and the input will also keep on adding new information as we encounter the cell state ct has two components one is this and the other is this the first component has forget gate and it is multiplied with the cell state of the previous timestamp this means that the forget gate will help us to decide what information to forget from the previous time step and what information to retain if the forget gate has not allowed to forget some information that means that we have still retained that information through a very very long time now this second component has input gate and the candidate value the candidate value tells us what new information can be potentially added and then this input gate will then decide if it wants to add that value or not the equations for the gates are given by this and as we have sigmoid here the values of these gates will be mostly either close to zero or either close to 1 and finally the hidden state will be updated with the help of this equation where the output gate is multiplied with the 10 h of c t this was regarding the one cell of lstm and this is how lstm look like throughout the time let's say this is our timestamp t equal to t1 this is t equal to 2 and this is t equal to t let's say then these cell state and the hidden state values will be passed on to the further time stem and this output prediction is made using this hidden state h t notice this is a neural network which means that the equation of ot will be given by applying soft max over w y multiplied by h t plus b y so this was everything regarding lstm i know it was a long video but i hope you learned something from it make sure you give the quiz which is down in the description box it will help you develop more understanding of this concept and remember what you have learned and if you found this video valuable then please do hit the like button it took me a lot of time to make this video share your support so that i keep on making such videos and up till then i will see you in the next video\n",
            "there are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory now in this video we will discuss what is lstm why do we need it and what is the problem with the recurrent neural network now this is the model which is a bit hard to understand and especially when you are looking at it or understanding it for the first time it does not quite make sense what is going on and same thing happened to me when i learned this concept for the first time i did not understand much so after going through a lot of resources i have wrapped everything into this single video which will not only give you the mathematical details behind lstm but also the intuition behind it which is very important to understand what is going on and i just forgot to tell you that at the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz and i hope that after watching this video you do not need to go anywhere else to understand lsdn and after watching this video if you understand lstm properly then do hit the like button show your support because it takes a lot of time and energy to make such kind of videos and you also know why should i repeat it again hit the red subscribe button also hit the bell icon so that you get notified every time i upload new video hit the red subscribe button and okay okay i know what you are saying stop talking joy start the video so let's get started let's say if we are making a grammar checker which checks if the given sentence is grammatically correct or not so this is a website called quiltbot which provides grammar checker and in this sentence if we read this sentence it says mr watson goes on a business trip every wednesday and one day during his summer break her wife was very angry now clearly this should be his not her and that's what the quillbot is suggesting us now this means that if we use a recurrent neural network for creating a grammar checker then this recurrent neural network should suggest us that here we should have his and not her because here the subject is mr watson which is a male but practically recurrent neural network are not able to do so let's see why let's say this is our recurrent neural network and we pass this sentence to it to check if it is grammatically correct or not now the individual words will go as an input to this recurrent neural network but when we pass this long sentence to the recurrent neural network it actually will fail to identify this as an error let's see why what happens in a recurring neural network is that these a is or which are activations or hidden state is passed from the previous timestamp to the next time stem till the end of the sentences and during each time stamp the value of the a are updated so by the time it reaches to this time stamp the value of the a has been updated so many times that it has almost lost the information that was present when it saw the word mr watson first that is why the problem with the recur neural network is that whenever we pass a long sentence it loses the information that was present very earlier in the sentence it remembers the information from the last few of the words but it fails to remember the words or the information that appeared long before now another way to prove this is by using the back propagation if you have watched my previous video on track propagation then you would know that we derived this del by del w with the help of this equation where which is the summation from i equal to 1 till all the time stem and here we have derivative of o which is the output with respect to the activation a and this activation a will be for all the time stem 1 till t so when we expand this we get this as an equation now this part represents the derivative of output o at the time stem t with respect to the activation or the hidden state at the timestamp 1 which means that when we are back propagating from the timestamp t till the time stem 1 we have to multiply lot of gradients to reach to the time stem 1 the value of the gradients are between 0 to 1 which means that the gradient value is a small number and whenever we multiply a lot of small numbers the net result becomes almost close to zero so the net result of this term will be almost equal to zero which means that while updating the weight which is actual training of our model the words that appear at the earlier stage has almost no say and this problem is known as vanishing gradient problem the word vanishing gradients means that the gradient value becomes so less that it becomes almost equal to zero and that is why we need another model that has a long term memory along with the short term memory in the rnn the value of the a's are updated at every time stem so they quickly lose the information that was provided back then and that is why they have short term memory but in lstm we have a long term memory as well as a short term memory and that is why the name is long short term memory here in lstm we have two states instead of just one state as we had in rnn the first state c here is called as a memory cell state and the second state here is called as hidden state this memory cell state will be responsible for retaining knowledge into its memory so that we can use it later on if we need it and this hidden state acts as the hidden state that we had in the recurrent neural network the long term memory will be able to retain the context or the knowledge that was provided along back and the main reason why the lstm is able to do that is because it used a concept of gates now there are three kinds of gates in lstm one is forget gate another is input gate and third is output gate now the concept of gate is that a gate allows certain information to pass through and it restricts the other information this means that a forget gate will be responsible for allowing our model to forget some information or retain some information so if it does not allow some context or some knowledge to forget then that knowledge will be retained throughout many time stem we will understand these in detail in just a while and the input gate will be responsible for adding new information so whenever our model sees new information provided by input x the input gate will allow some information to be added to our network while it won't allow some information to be added so it will only allow useful information to add to our model and the output gate will be responsible for letting us know what to produce as an output let's look at them in detail so let's look at every component one by one we will first start with looking at our memory cell state ct in the equation of ct it has two components one have forget gate and other have input gate let's see first what is the role of forget gate here now if you see here the forget gate is multiplied with the c from the previous time stem and this is an element-wise multiplication which means that the shape of ft will be same as the shape of c t minus 1 and the values inside the matrix f t will be 0 or 1 so the values of the forget gate matrix will be multiplied with the values with the c t minus 1 and as it is the element wise multiplication which means that this value will be multiplied with this this will be multiplied with this and so on and that is why let's say when we have one here in the forget gate it will retain that information or it will retain that value and whenever we have zero in the forget gate then it will eliminate that information or that value and that is why it acts as a gate 1 means allowed to pass through while 0 means discard that information and that is why it is called forget git because it can forget some information and now let's see why do we want to forget some information let's say we have this long sentence here whenever a model sees mr watson it will retain that information or this context into the cell state c1 now c will be a matrix right so some of its value will retain the information that we have mr watson here and now the forget gate will not allow the information here to change that is why the information will retain throughout in the simple recurrent neural network these values would have updated quickly as our model sees the next word but it does not happen in lstm and that is why let's say when it reaches at this world her it recognized that here it should be his wife and not her wife because the sea has still stored mr watson in its memory and let's say we encounter some other word miss mary then now the forget gate will forget the information of mr watson but it will add the information of miss mary and that is why we have this input gate i t here and as it has forgotten that we had mr watson and it has a remake and as it has retained that we have miss mary now then our model can decide that here we should have herself and not himself because miss mary is a female now we will understand the second term of our ct which is this part this has an input gate i t and this c tilde t is known as candidate value the equation for the candidate value is given by this now the equation of the candidate value is almost the same as the equation of a that we had in recurrent neural network in the simple recurrent neural network now if you remember the equation of the a in simple recurrent neural network was given by this which can also be represented as this here what i have done is that i have combined the weight matrix into wa and i have concatenated the 80 minus 1 and x t this means that the weight matrix w a here is a combination of w a a and w a x and a t minus 1 is concatenated vertically with x t and thus when we multiply this we will get w a a 80 minus 1 and this equation back so what i wanted to say is that these two equations are the same and here this represent that a t minus 1 is concatenated with x t and now we have the w a matrix which has both w a a and w a x this means that the s equation of a t is similar to the our candidate value here this candidate value will be responsible for adding new information and as its name suggests that it's a candidate value which means that potential new information that we can add and that potential new information will be filtered by this input gate this means that the input gate i t will also have the 0 and 1 as its values and thus it will filter out what new information it wants to add let us understand this with the help of an example the statement says that mr watson goes on a business trip every wednesday and one day during a summer break harvard was very angry and tomorrow he was need to go on a trip again now clearly this should be his and this should be will because here we have tomorrow the proper statement would have been and tomorrow he will need to go on a trip again so whenever our model reaches this word tomorrow it also needs to add that information along with whatever information it had before so this is an example where we do not need to forget anything but we are just adding new information while retaining the useful information that we had before thus this model has the capacity to retain the old information for a long time along with adding new information now we discussed that the gate stores 0 and 1 as its value but let's see the equation of these input gate and the forget gate as well the equation of the forget gate and the input gate is given by this you will notice that this component is almost same in all of these the only difference is of the weights here we have wc here we have wf here we have wi and similarly for the biases as well and here we are applying sigmoid activation function now as it is a sigmoid activation function the values of the ft will be between 0 and 1 only so most of its value will be either close to 0 or either close to 1. so if the value is close to 0 it means that we want to forget that information and if the value is close to 1 that means that we want to retain that information and that is why they are able to act as gates now you might ask me that hey jay how does these gates get to know which information to forget and which information to add and the answer lies in the weight matrices here notice that we will train this model and after training the values of these weights will be updated in such a way that it will build a certain level of understanding which means that after training our model will develop an understanding of which information is useful and which information is irrelevant and this understanding is built after looking at thousands and thousands of data or sentences now these equations diagrammatically look something like this here we will provide the input word which will be provided at every time stem here we have hidden state and here we have cell state now this box here states that it is a neural network a neural network means that the hidden state and the input x will be multiplied with the weights and it will be passed on to some activation function and and then we will have this forget gate which will be multiplied with ct minus 1. so we have forget gate multiplied by ct minus 1 and an input gate multiplied by candidate value which is obtained from here now we have looked at two gates now it's time to look at our third gate which is the output gate the value of the hidden state ht will be computed by multiplying the output gate and by passing the tan h over c t and this hidden state will be provided as an output to make the prediction the equation of the output gate is similar to the equation of the forget and the input gate the only difference is that we have the weight w0 and the bias be 0. now this output gate will filter out what information to be predicted or given as the output by what information to be not given in the output now i know we discussed a lot of things here so a quick revision will be very very useful here so let us revise everything that we have learned so far lstm will have two states one is the cell state and the other is the hidden state the cell state will act as a long term memory which will be able to retain some information through long time and with the help of the hidden state and the input will also keep on adding new information as we encounter the cell state ct has two components one is this and the other is this the first component has forget gate and it is multiplied with the cell state of the previous timestamp this means that the forget gate will help us to decide what information to forget from the previous time step and what information to retain if the forget gate has not allowed to forget some information that means that we have still retained that information through a very very long time now this second component has input gate and the candidate value the candidate value tells us what new information can be potentially added and then this input gate will then decide if it wants to add that value or not the equations for the gates are given by this and as we have sigmoid here the values of these gates will be mostly either close to zero or either close to 1 and finally the hidden state will be updated with the help of this equation where the output gate is multiplied with the 10 h of c t this was regarding the one cell of lstm and this is how lstm look like throughout the time let's say this is our timestamp t equal to t1 this is t equal to 2 and this is t equal to t let's say then these cell state and the hidden state values will be passed on to the further time stem and this output prediction is made using this hidden state h t notice this is a neural network which means that the equation of ot will be given by applying soft max over w y multiplied by h t plus b y so this was everything regarding lstm i know it was a long video but i hope you learned something from it make sure you give the quiz which is down in the description box it will help you develop more understanding of this concept and remember what you have learned and if you found this video valuable then please do hit the like button it took me a lot of time to make this video share your support so that i keep on making such videos and up till then i will see you in the next video\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "There are a few problems with recurrent neural network which is solved by this model named lstm which stands for long short term memory. At the end of this video you will find a link to the quiz down in the description box the quiz is very important to retain your understanding and improve your understanding so make sure you attempt the quiz. After watching this video do hit the like button show your support because it takes a lot of time and energy to make such kind of videos.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTIPLE FILE PROCESSING\n",
        "AT A TIME MULTIPLE FILES ARE INPUT\n"
      ],
      "metadata": {
        "id": "Ugi8aDYG6w4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGuxlXs_5aRu",
        "outputId": "8d6edd62-3bc6-4d37-cdd0-7559607cea79"
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ RAG Chatbot - Upload Files or URLs\n",
            "Enter file path or URL (or 'done' to finish): /content/Students_Grading_Dataset.csv\n",
            "‚úÖ Processing /content/Students_Grading_Dataset.csv ...\n",
            "‚úÖ Successfully processed /content/Students_Grading_Dataset.csv.\n",
            "Enter file path or URL (or 'done' to finish): /content/Data Science Interview.pdf\n",
            "‚úÖ Processing /content/Data Science Interview.pdf ...\n",
            "‚úÖ Successfully processed /content/Data Science Interview.pdf.\n",
            "Enter file path or URL (or 'done' to finish): /content/db.docx\n",
            "‚úÖ Processing /content/db.docx ...\n",
            "‚úÖ Successfully processed /content/db.docx.\n",
            "Enter file path or URL (or 'done' to finish): done\n",
            "\n",
            "‚úÖ Index Built Successfully!\n",
            "\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): explain all\n",
            "\n",
            "üìå **Top Retrieved Contexts:**\n",
            "These scenarios demonstrate how to think about identifying causal chains that may create context-\n",
            "specific data incidents.\n",
            "4 IAPP, Is It an Incident or a Breach, How to Tell and Why It Matters, Mahmoud Sher-Jan (February 2017).\n",
            "‚Ä¢ The International Organization for Standardization‚Äôs (ISO) in ISO Standard 27000 defines a ‚Äòcritical\n",
            "incident‚Äô as ‚Äúa single or a series of unwanted or unexpected information security events that have a\n",
            "significant probability of compromising business operations and threatening information security.‚Äù2\n",
            "‚Ä¢ The United States Department of Commerce National Institute for Standards and Technology (NIST)\n",
            "defines an adverse event involving a ‚Äòcyber threat‚Äô as ‚Äú[a]n event or condition that has the potential\n",
            "for causing asset loss and the undesirable consequences or impact from such loss.‚Äù3\n",
            "‚Ä¢ Mahmood Sher-Jan of the International Association of Privacy Professionals (IAPP) identifies three\n",
            "additional categories of events that expand upon the NIST definition of adverse events.\n",
            "\n",
            "üìå **Abstractive Summary:**\n",
            "The scenarios demonstrate how to think about identifying causal chains that may create context-specific data incidents. These scenarios demonstrate the different ways that data can be misused. The scenarios are based on the International Organization for Standardization‚Äôs ( ISO) in ISO Standard 27000 defines a ‚Äòcritical‚Äô data incident.\n",
            "\n",
            "üîç Enter your query (or 'exit' to quit): exit\n"
          ]
        }
      ]
    }
  ]
}